# ruff: noqa: E501
from __future__ import annotations

import codecs
import logging
import os
import re
import unicodedata
from collections.abc import Callable, Iterable, Iterator, Mapping
from dataclasses import dataclass
from difflib import SequenceMatcher
from re import Match
from typing import Any, cast

SECTION_BLOCK_PATTERN = re.compile(r"(^#+.*?)(?=^#+ |\Z)", re.MULTILINE | re.DOTALL)


# ğŸ”§ Phase 1ä¿®å¤ï¼šå®šä¹‰åŒ¹é…å¸¸é‡ï¼Œæ›¿æ¢é­”æ³•æ•°å­—
class MatchingThresholds:
    """å¥å­åŒ¹é…çš„é˜ˆå€¼å¸¸é‡"""

    # ç›¸ä¼¼åº¦é˜ˆå€¼
    SIMILARITY_STRICT = 0.60  # ä¸¥æ ¼ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆé»˜è®¤ï¼‰
    SIMILARITY_MODERATE = 0.50  # ä¸­ç­‰ç›¸ä¼¼åº¦åŒ¹é…
    SIMILARITY_RELAXED = 0.40  # å®½æ¾ç›¸ä¼¼åº¦åŒ¹é…
    SIMILARITY_FUZZY = 0.35  # æ¨¡ç³ŠåŒ¹é…æœ€ä½é˜ˆå€¼

    # è¦†ç›–ç‡é˜ˆå€¼
    COVERAGE_MINIMUM = 0.6  # æ ¸å¿ƒè¯æ±‡æœ€å°è¦†ç›–ç‡ï¼ˆ60%ï¼‰

    # ç»¼åˆè¯„åˆ†é˜ˆå€¼
    COMBINED_SCORE_MIN = 0.45  # ç»¼åˆåˆ†æ•°æœ€ä½è¦æ±‚

    # æ¨¡ç³ŠåŒ¹é…å‚æ•°
    MIN_FUZZY_LENGTH = 15  # æ¨¡ç³ŠåŒ¹é…æœ€å°å­—ç¬¦ä¸²é•¿åº¦


# æ³¨æ„ï¼šè¡¥ä¸è½½è·æ¥è‡ªå¤–éƒ¨JSONï¼Œå› æ­¤å€¼ä¿æŒä¸º`Any`ç±»å‹
# åœ¨æ­¤é€‚é…å±‚ä¸­ï¼Œåœ¨ä¸‹æ¸¸éªŒè¯ä¹‹å‰ä¿æŒçµæ´»æ€§
JsonDict = dict[str, Any]
CorrectionResult = tuple[str, str, float, str, str, int]
Strategy = Callable[[str, str, int], CorrectionResult | None]


def _normalize_mapping(mapping: Any) -> JsonDict:
    if not isinstance(mapping, Mapping):
        return {}
    typed_mapping = cast(Mapping[Any, Any], mapping)
    normalized: JsonDict = {}
    for key, value in typed_mapping.items():
        normalized[str(key)] = value
    return normalized


def _find_section_by_id(document: str, section_id: str) -> Match[str] | None:
    """
    å®šä½ç”±æ³¨å…¥çš„section_idæ³¨é‡Šæ ‡è¯†çš„Markdownç« èŠ‚å—ã€‚
    ä¼˜å…ˆåŒ¹é…åŒ…å«<!-- section_id: ... -->æ ‡è®°çš„æ ‡é¢˜ã€‚
    """
    if not document or not section_id:
        return None

    escaped_id = re.escape(section_id)
    direct_pattern = re.compile(
        rf"(^#+.*?<!--\s*section_id:\s*{escaped_id}\s*-->.*?)(?=^#+ |\Z)",
        re.MULTILINE | re.DOTALL,
    )
    match = direct_pattern.search(document)
    if match:
        return match

    comment_pattern = re.compile(rf"<!--\s*section_id:\s*{escaped_id}\s*-->")
    comment_match = comment_pattern.search(document)
    if not comment_match:
        return None

    heading_pattern = re.compile(r"^#+.*", re.MULTILINE)
    heading_match: Match[str] | None = None
    for candidate in heading_pattern.finditer(document, 0, comment_match.start()):
        heading_match = candidate
    if not heading_match:
        return None

    return SECTION_BLOCK_PATTERN.search(document, heading_match.start())


def _find_section_by_title(document: str, section_title: str) -> Match[str] | None:
    """
    é€šè¿‡ç« èŠ‚æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾ç« èŠ‚

    Args:
        document: æ–‡æ¡£å†…å®¹
        section_title: ç« èŠ‚æ ‡é¢˜

    Returns:
        åŒ¹é…çš„ç« èŠ‚ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    if not document or not section_title:
        return None

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    clean_title = re.sub(r"[^\w\s]", "", section_title)

    # å°è¯•åŒ¹é…åŒ…å«è¯¥æ ‡é¢˜çš„markdownæ ‡é¢˜è¡Œ
    title_pattern = rf"(^##+\s+.*?{re.escape(clean_title)}.*?)(?=^#+ |\Z)"
    match = re.search(title_pattern, document, re.MULTILINE | re.DOTALL | re.IGNORECASE)

    if match:
        return match

    return None


def _list_available_section_ids(document: str) -> list[str]:
    """
    åˆ—å‡ºæ–‡æ¡£ä¸­æ‰€æœ‰å¯ç”¨çš„section_id

    Args:
        document: æ–‡æ¡£å†…å®¹

    Returns:
        section_idåˆ—è¡¨
    """
    pattern = r"<!--\s*section_id:\s*([^\s]+)\s*-->"
    matches = re.findall(pattern, document)
    return matches


def _find_section_by_multiple_methods(
    document: str,
    section_id: str,
    section_title: str | None = None,
) -> tuple[Match[str] | None, str | None]:
    """
    é€šè¿‡å¤šç§æ–¹å¼æŸ¥æ‰¾ç« èŠ‚

    Args:
        document: æ–‡æ¡£å†…å®¹
        section_id: ç›®æ ‡ç« èŠ‚ID
        section_title: ç« èŠ‚æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºé™çº§åŒ¹é…ï¼‰

    Returns:
        (åŒ¹é…çš„ç« èŠ‚, åŒ¹é…æ–¹å¼æè¿°)
    """
    # æ–¹æ³•1: é€šè¿‡section_idæ³¨é‡Šç²¾ç¡®åŒ¹é…
    match = _find_section_by_id(document, section_id)
    if match:
        return match, "section_id"

    # æ–¹æ³•2: é€šè¿‡ç« èŠ‚æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…
    if section_title:
        match = _find_section_by_title(document, section_title)
        if match:
            logging.info(f"é€šè¿‡æ ‡é¢˜åŒ¹é…æ‰¾åˆ°ç« èŠ‚: {section_title}")
            return match, "title"

    # æ–¹æ³•3: åˆ—å‡ºå¯ç”¨çš„section_idå¸®åŠ©è¯Šæ–­
    available_ids = _list_available_section_ids(document)
    logging.error(f"ç« èŠ‚IDæ˜ å°„å¤±è´¥ã€‚ç›®æ ‡: {section_id}")
    if available_ids:
        logging.error(f"å¯ç”¨section_id (å‰10ä¸ª): {available_ids[:10]}")
    else:
        logging.error("æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ä»»ä½•section_idæ³¨é‡Š")

    return None, None


def _normalize_sentence_tokens(sentence: str) -> list[str]:
    return [token for token in re.split(r"\s+", sentence.strip()) if token]


def _find_section_containing_offset(document: str, offset: int) -> Match[str] | None:
    if offset < 0:
        return None
    for match in SECTION_BLOCK_PATTERN.finditer(document):
        if match.start() <= offset < match.end():
            return match
    return None


def _find_section_by_sentences(document: str, sentences: Iterable[str]) -> Match[str] | None:
    """
    å°è¯•é€šè¿‡æ‰«ææä¾›çš„å¥å­ç‰‡æ®µæ¥å®šä½ç« èŠ‚ã€‚
    å½“ç« èŠ‚IDå‘ç”Ÿæ¼‚ç§»ä½†å†…å®¹ä»å­˜åœ¨æ—¶å¾ˆæœ‰ç”¨ã€‚
    """
    processed: list[tuple[list[str], str]] = []
    seen_keys: set[str] = set()
    for sentence in sentences:
        if not sentence:
            continue
        cleaned = sentence.strip()
        if not cleaned:
            continue
        tokens = _normalize_sentence_tokens(cleaned)
        key = " ".join(tokens)
        if not tokens or key in seen_keys:
            continue
        seen_keys.add(key)
        processed.append((tokens, cleaned))
    processed.sort(key=lambda item: len(item[1]), reverse=True)

    for tokens, original_sentence in processed:
        if not tokens:
            continue
        literal = original_sentence
        literal_index = document.find(literal)
        if literal_index != -1:
            section_match = _find_section_containing_offset(document, literal_index)
            if section_match:
                return section_match
        pattern = r"\s+".join(re.escape(token) for token in tokens)
        if not pattern:
            continue
        regex = re.compile(pattern, re.MULTILINE | re.IGNORECASE)
        regex_match = regex.search(document)
        if regex_match:
            section_match = _find_section_containing_offset(document, regex_match.start())
            if section_match:
                return section_match
    return None


@dataclass
class EditIntent:
    original_sentence: str
    revised_sentence: str
    expected_replacements: int = 1
    metadata: dict[str, Any] | None = None


@dataclass
class EditOutcome:
    applied: bool
    method: str
    similarity: float | None
    matched_fragment: str | None
    detail: str
    replacements: int = 0


@dataclass
class _TextSegment:
    raw_text: str
    clean_text: str
    start: int
    end: int
    label: str


@dataclass
class FineGrainedEditResult:
    """
    å•æ¬¡ç»†ç²’åº¦ç¼–è¾‘åº”ç”¨çš„æ±‡æ€»ç»Ÿè®¡ã€‚
    """

    updated_text: str
    sections_modified: int
    successful_edits: int
    failed_edits: list[str]
    total_replacements: int = 0

    @property
    def had_effect(self) -> bool:
        return self.sections_modified > 0 or self.successful_edits > 0


class EditCorrector:
    """
    å¥å­çº§ç¼–è¾‘çš„ç²¾ç¡®ä¼˜å…ˆæ›¿æ¢å·¥å…·ã€‚
    """

    _SENTENCE_PATTERN = re.compile(r"[^ã€‚ï¼ï¼Ÿ!?\n]*[ã€‚ï¼ï¼Ÿ!?]+|[^\n]+", re.MULTILINE)

    def __init__(
        self,
        body_text: str,
        *,
        similarity_threshold: float = 0.65,  # é™ä½é»˜è®¤é˜ˆå€¼ä»¥æé«˜åŒ¹é…æˆåŠŸç‡
        min_fuzzy_length: int = 15,  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
    ):
        self._text = body_text
        self.similarity_threshold = max(0.4, min(similarity_threshold, 0.95))  # æ›´å®½æ¾çš„èŒƒå›´
        self.min_fuzzy_length = max(5, min_fuzzy_length)  # æ›´ä½çš„æœ€å°é•¿åº¦

    @property
    def text(self) -> str:
        return self._text

    def apply(self, intent: EditIntent) -> EditOutcome:
        original = (intent.original_sentence or "").strip()
        revised = intent.revised_sentence or ""
        expected = intent.expected_replacements or 1

        if not original or not revised.strip():
            return EditOutcome(
                applied=False,
                method="invalid",
                similarity=None,
                matched_fragment=None,
                detail="Empty original or revised sentence.",
            )

        if original == revised:
            return EditOutcome(
                applied=False,
                method="noop",
                similarity=1.0,
                matched_fragment=original,
                detail="Original and revised sentences are identical.",
            )

        candidate_pairs: list[tuple[str, str]] = [(original, revised)]

        display_original = self._convert_inline_math_to_display(original)
        display_revised = self._convert_inline_math_to_display(revised)
        if (display_original, display_revised) != (original, revised):
            candidate_pairs.append((display_original, display_revised))

        last_outcome: EditOutcome | None = None
        for candidate_original, candidate_revised in candidate_pairs:
            outcome = self._attempt_strategies(candidate_original, candidate_revised, expected)
            if outcome.applied:
                return outcome
            last_outcome = outcome

        return last_outcome or EditOutcome(
            applied=False,
            method="unmatched",
            similarity=None,
            matched_fragment=None,
            detail="No matching snippet satisfied similarity/occurrence guards.",
        )

    def _attempt_strategies(self, original: str, revised: str, expected: int) -> EditOutcome:
        strategies: tuple[Strategy, ...] = (
            self._try_literal,
            self._try_whitespace_normalised,
            self._try_unescaped_literal,
            self._try_casefold_literal,
            self._try_math_normalised_segment,
            self._try_segment_similarity,
            self._try_fuzzy_window,
            self._try_relaxed_segment_similarity,
            self._try_substring_fuzzy_match,  # ğŸ†• æ–°å¢ï¼šæœ€åçš„é™çº§ç­–ç•¥
        )

        for strategy in strategies:
            result = strategy(original, revised, expected)
            if result:
                new_text, method, score, fragment, detail, replacements = result
                if new_text != self._text:
                    self._text = new_text
                return EditOutcome(True, method, score, fragment, detail, replacements)

        return EditOutcome(
            applied=False,
            method="unmatched",
            similarity=None,
            matched_fragment=None,
            detail="No matching snippet satisfied similarity/occurrence guards.",
        )

    # --- Strategy implementations -------------------------------------------------

    def _try_literal(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        occurrences = self._text.count(original)
        if occurrences == 0:
            return None
        if occurrences > 1 and expected <= 1:
            logging.debug(
                "Literal replacement skipped because the target sentence appears %s times; deferring to contextual strategies.",
                occurrences,
            )
            return None

        max_replacements = expected if expected > 0 else occurrences
        new_text, replacements = self._safe_literal_replace(self._text, original, revised, max_replacements)
        if replacements == 0:
            return None
        if expected > 0 and replacements < expected:
            logging.debug(
                "Literal replacement applied %s/%s expected occurrences; proceeding with partial match.",
                replacements,
                expected,
            )

        return (
            new_text,
            "literal",
            1.0,
            original,
            f"Replaced literal match ({replacements}).",
            replacements,
        )

    def _try_whitespace_normalised(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        pattern = self._build_whitespace_flexible_pattern(original)
        matches = list(re.finditer(pattern, self._text, flags=re.MULTILINE))
        if len(matches) != 1:
            logging.debug(
                "Whitespace-normalised search matched %s fragments; expected singular.",
                len(matches),
            )
            return None

        match = matches[0]
        fragment = match.group(0)
        score = self._similarity(original, fragment)
        new_text = self._replace_span(match.start(), match.end(), revised)
        return (
            new_text,
            "whitespace_normalised",
            score,
            fragment,
            "Whitespace-normalised literal replacement.",
            1,
        )

    def _try_unescaped_literal(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        unescaped_original, changed = self._maybe_unescape_literal(original)
        if not changed:
            return None

        occurrences = self._text.count(unescaped_original)
        if occurrences == 0:
            return None

        unescaped_revised, revised_changed = self._maybe_unescape_literal(revised)
        replacement = unescaped_revised if revised_changed else revised
        max_replacements = expected if expected > 0 else occurrences
        new_text, replacements = self._safe_literal_replace(self._text, unescaped_original, replacement, max_replacements)
        if replacements == 0:
            return None
        if expected > 0 and replacements < expected:
            logging.debug(
                "Unescaped literal replacement applied %s/%s expected occurrences; proceeding with partial match.",
                replacements,
                expected,
            )

        score = self._similarity(original, unescaped_original)
        return (
            new_text,
            "unescaped_literal",
            score,
            unescaped_original,
            "Literal replacement after correcting escape sequences.",
            replacements,
        )

    def _try_casefold_literal(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        original_cf = original.casefold()
        text_cf = self._text.casefold()
        idx = text_cf.find(original_cf)
        if idx == -1:
            return None
        second_idx = text_cf.find(original_cf, idx + len(original_cf))
        if second_idx != -1:
            logging.debug("Casefold match not unique; skipping.")
            return None
        fragment = self._text[idx : idx + len(original)]
        new_text = self._replace_span(idx, idx + len(original), revised)
        return (
            new_text,
            "casefold_literal",
            1.0,
            fragment,
            "Case-insensitive literal replacement.",
            1,
        )

    def _try_math_normalised_segment(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        stripped_original = self._strip_math_expressions(original)
        if len(stripped_original.strip()) < 8:
            return None

        best_segment: _TextSegment | None = None
        best_score = 0.0

        for segment in self._iter_segments():
            if self._should_skip_segment(segment):
                continue
            stripped_segment = self._strip_math_expressions(segment.clean_text)
            if not stripped_segment.strip():
                continue
            score = self._similarity(stripped_original, stripped_segment)
            if score >= max(self.similarity_threshold - 0.2, 0.45) and score > best_score:  # æ›´å®½æ¾çš„åŒ¹é…
                best_segment = segment
                best_score = score

        if not best_segment:
            return None

        replacement = self._preserve_whitespace(best_segment.raw_text, revised)
        new_text = self._replace_span(best_segment.start, best_segment.end, replacement)
        return (
            new_text,
            f"segment_math_{best_segment.label}",
            best_score,
            best_segment.raw_text,
            f"Math-normalised segment similarity {best_score:.2f}",
            1,
        )

    def _try_segment_similarity(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        best_segment: _TextSegment | None = None
        best_score = 0.0

        for segment in self._iter_segments():
            if self._should_skip_segment(segment):
                continue
            score = self._similarity(original, segment.clean_text)
            if score >= self.similarity_threshold and score > best_score:
                best_segment = segment
                best_score = score

        if not best_segment:
            return None

        replacement = self._preserve_whitespace(best_segment.raw_text, revised)
        new_text = self._replace_span(best_segment.start, best_segment.end, replacement)
        return (
            new_text,
            f"segment_{best_segment.label}",
            best_score,
            best_segment.raw_text,
            f"Segment similarity {best_score:.2f}",
            1,
        )

    def _try_fuzzy_window(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        norm_len = len(self._normalize_for_similarity(original))
        if norm_len < self.min_fuzzy_length:
            return None

        best_span: tuple[int, int, float] | None = None
        for start, end in self._candidate_paragraph_spans(original):
            refined_start, refined_end, score = self._refine_span(start, end, original)
            if score >= max(self.similarity_threshold - 0.15, 0.5):  # æ›´å®½æ¾çš„æ¨¡ç³Šçª—å£åŒ¹é…
                if not best_span or score > best_span[2]:
                    best_span = (refined_start, refined_end, score)

        if not best_span:
            return None

        span_start, span_end, score = best_span
        fragment = self._text[span_start:span_end]
        replacement = self._preserve_whitespace(fragment, revised)
        new_text = self._replace_span(span_start, span_end, replacement)
        return (
            new_text,
            "fuzzy_window",
            score,
            fragment,
            f"Fuzzy window similarity {score:.2f}",
            1,
        )

    def _try_relaxed_segment_similarity(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        """
        Final safety net: choose the most similar segment even if it falls below the
        strict similarity threshold, provided it is still reasonably close.
        """
        best_segment: _TextSegment | None = None
        best_score = 0.0

        for segment in self._iter_segments():
            if self._should_skip_segment(segment):
                continue
            score = self._similarity(original, segment.clean_text)
            if score > best_score:
                best_score = score
                best_segment = segment

        # è¦æ±‚å®½æ¾ä½†éå¹³å‡¡çš„åŒ¹é…ï¼Œé¿å…è™šæ„çš„æ›¿æ¢
        relaxed_threshold = max(self.similarity_threshold - 0.35, 0.4)  # æ›´å®½æ¾çš„æœ€ç»ˆå›é€€ç­–ç•¥
        if not best_segment or best_score < relaxed_threshold:
            return None

        replacement = self._preserve_whitespace(best_segment.raw_text, revised)
        new_text = self._replace_span(best_segment.start, best_segment.end, replacement)
        return (
            new_text,
            f"segment_relaxed_{best_segment.label}",
            best_score,
            best_segment.raw_text,
            f"Relaxed segment similarity {best_score:.2f}",
            1,
        )

    def _try_substring_fuzzy_match(self, original: str, revised: str, expected: int) -> CorrectionResult | None:
        """
        æœ€åçš„é™çº§ç­–ç•¥ï¼šåŸºäºå­ä¸²å’Œå…³é”®è¯çš„æ¨¡ç³ŠåŒ¹é…

        å½“æ‰€æœ‰å…¶ä»–ç­–ç•¥éƒ½å¤±è´¥æ—¶ï¼Œå°è¯•é€šè¿‡æ ¸å¿ƒè¯æ±‡è¦†ç›–ç‡æ¥åŒ¹é…ã€‚
        è¿™æ˜¯ä¸ºäº†å¤„ç† AI ç”Ÿæˆçš„åŸå¥ä¸æ–‡æ¡£æœ‰è½»å¾®å·®å¼‚çš„æƒ…å†µã€‚
        """
        if len(original) < self.min_fuzzy_length:
            return None

        # æå–æ ¸å¿ƒè¯æ±‡ï¼ˆé•¿åº¦>=3çš„è¯ï¼Œé¿å…åœç”¨è¯ï¼‰
        core_words = [word for word in re.findall(r"\w{3,}", original) if word]
        if not core_words or len(core_words) < 3:
            return None

        best_segment = None
        best_score = 0.0
        best_coverage = 0.0

        for segment in self._iter_segments():
            if self._should_skip_segment(segment):
                continue

            segment_text = segment.clean_text.lower()

            # è®¡ç®—æ ¸å¿ƒè¯æ±‡è¦†ç›–ç‡
            matches = sum(1 for word in core_words if word.lower() in segment_text)
            coverage = matches / len(core_words)

            # è‡³å°‘éœ€è¦60%çš„æ ¸å¿ƒè¯åŒ¹é…
            if coverage >= 0.6:
                similarity = self._similarity(original, segment.clean_text)

                # ç»¼åˆè€ƒè™‘è¦†ç›–ç‡å’Œç›¸ä¼¼åº¦
                combined_score = coverage * 0.4 + similarity * 0.6

                if combined_score > best_score:
                    best_segment = segment
                    best_score = combined_score
                    best_coverage = coverage

        # æ›´å®½æ¾çš„é˜ˆå€¼ï¼šç»¼åˆåˆ†æ•° >= 0.45 æˆ–ç›¸ä¼¼åº¦ >= 0.35
        if best_segment and (best_score >= 0.45 or self._similarity(original, best_segment.clean_text) >= 0.35):
            actual_similarity = self._similarity(original, best_segment.clean_text)
            logging.info("    Â· ä½¿ç”¨å­ä¸²æ¨¡ç³ŠåŒ¹é… (coverage=%.0f%%, similarity=%.2f, combined=%.2f)", best_coverage * 100, actual_similarity, best_score)

            replacement = self._preserve_whitespace(best_segment.raw_text, revised)
            new_text = self._replace_span(best_segment.start, best_segment.end, replacement)

            if new_text == self._text:
                return None

            return (
                new_text,
                f"substring_fuzzy_{best_segment.label}",
                actual_similarity,
                best_segment.raw_text,
                f"Substring fuzzy match (coverage={best_coverage:.0%}, sim={actual_similarity:.2f})",
                1,
            )

        return None

    # --- Helper utilities --------------------------------------------------------

    @staticmethod
    def _safe_literal_replace(text: str, needle: str, replacement: str, max_replacements: int) -> tuple[str, int]:
        if not needle:
            return text, 0

        limit = max_replacements if max_replacements > 0 else 1
        pieces: list[str] = []
        start = 0
        replaced = 0

        while True:
            idx = text.find(needle, start)
            if idx == -1 or replaced >= limit:
                pieces.append(text[start:])
                break
            pieces.append(text[start:idx])
            pieces.append(replacement)
            replaced += 1
            start = idx + len(needle)

        return "".join(pieces), replaced

    def _replace_span(self, start: int, end: int, replacement: str) -> str:
        return self._text[:start] + replacement + self._text[end:]

    @staticmethod
    def _maybe_unescape_literal(text: str) -> tuple[str, bool]:
        if not text or "\\" not in text:
            return text, False
        try:
            candidate = codecs.decode(text, "unicode_escape")
        except Exception:
            return text, False
        if candidate == text:
            return text, False
        return candidate, True

    @staticmethod
    def _normalize_for_similarity(text: str) -> str:
        normalized = unicodedata.normalize("NFKC", text or "")
        normalized = normalized.casefold()
        normalized = re.sub(r"\s+", " ", normalized)
        return normalized.strip()

    def _similarity(self, left: str, right: str) -> float:
        left_norm = self._normalize_for_similarity(left)
        right_norm = self._normalize_for_similarity(right)
        if not left_norm or not right_norm:
            return 0.0
        return SequenceMatcher(None, left_norm, right_norm).ratio()

    @staticmethod
    def _build_whitespace_flexible_pattern(text: str) -> str:
        parts = [re.escape(part) for part in re.split(r"\s+", text.strip()) if part]
        return r"\s*".join(parts)

    @staticmethod
    def _strip_math_expressions(text: str) -> str:
        if not text:
            return ""
        # å…ˆç§»é™¤å—çº§æ•°å­¦è¡¨è¾¾å¼
        cleaned = re.sub(r"\$\$.*?\$\$", " ", text, flags=re.DOTALL)
        # å†ç§»é™¤è¡Œå†…æ•°å­¦è¡¨è¾¾å¼
        cleaned = re.sub(r"\$(?:\\.|[^$])+\$", " ", cleaned)
        # ç§»é™¤å¤šä½™ç©ºç™½
        cleaned = re.sub(r"\s+", " ", cleaned)
        return cleaned.strip()

    @staticmethod
    def _convert_inline_math_to_display(text: str) -> str:
        if not text or "$" not in text or "$$" in text:
            return text

        pattern = re.compile(r"\$(.+?)\$", re.DOTALL)

        def _to_display(match: re.Match[str]) -> str:
            expr = match.group(1)
            if expr is None:
                return match.group(0)
            stripped = expr.strip()
            if not stripped:
                return match.group(0)
            if "=" not in stripped and len(stripped) <= 40:
                return match.group(0)
            return f"\n\n$${stripped}$$\n\n"

        converted = pattern.sub(_to_display, text)
        if converted != text:
            converted = re.sub(r"\n{3,}", "\n\n", converted)
        return converted

    def _iter_segments(self) -> Iterator[_TextSegment]:
        yield from self._line_window_segments()
        yield from self._sentence_segments()

    def _line_window_segments(self) -> Iterator[_TextSegment]:
        lines = self._text.splitlines(True)
        offsets = [0]
        for line in lines:
            offsets.append(offsets[-1] + len(line))

        max_window = 3
        seen: set[tuple[int, int]] = set()
        for i in range(len(lines)):
            for width in range(1, max_window + 1):
                if i + width > len(lines):
                    break
                start = offsets[i]
                end = offsets[i + width]
                span = (start, end)
                if span in seen:
                    continue
                seen.add(span)
                raw = self._text[start:end]
                clean = raw.strip()
                if not clean:
                    continue
                yield _TextSegment(raw, clean, start, end, f"line_window_{width}")

    def _sentence_segments(self) -> Iterator[_TextSegment]:
        seen: set[tuple[int, int]] = set()
        for match in self._SENTENCE_PATTERN.finditer(self._text):
            start, end = match.start(), match.end()
            span = (start, end)
            if span in seen:
                continue
            seen.add(span)
            raw = self._text[start:end]
            clean = raw.strip()
            if not clean:
                continue
            yield _TextSegment(raw, clean, start, end, "sentence")

    @staticmethod
    def _should_skip_segment(segment: _TextSegment) -> bool:
        clean = segment.clean_text.lstrip()
        if not clean:
            return True
        if clean.startswith("#"):
            return True
        if "section_id:" in segment.raw_text:
            return True
        return False

    @staticmethod
    def _preserve_whitespace(original_fragment: str, revised: str) -> str:
        leading_match = re.match(r"^\s*", original_fragment)
        trailing_match = re.search(r"\s*$", original_fragment)
        leading = leading_match.group(0) if leading_match else ""
        trailing = trailing_match.group(0) if trailing_match else ""
        stripped_revised = revised.strip("\n")
        replacement = stripped_revised
        if leading and not replacement.startswith(leading):
            replacement = leading + replacement
        if trailing and not replacement.endswith(trailing):
            replacement = replacement + trailing
        return replacement

    def _candidate_paragraph_spans(self, original: str) -> Iterator[tuple[int, int]]:
        tokens = [tok for tok in re.findall(r"\w+", original) if len(tok) > 3]
        token_pattern = re.compile("|".join(re.escape(tok) for tok in set(tokens)), re.IGNORECASE) if tokens else None
        pattern = re.compile(r".+?(?:\n\s*\n|\Z)", re.DOTALL)
        for match in pattern.finditer(self._text):
            raw = match.group(0)
            if token_pattern and not token_pattern.search(raw):
                continue
            start, end = match.start(), match.end()
            if raw.strip():
                yield start, end

    def _refine_span(self, start: int, end: int, original: str) -> tuple[int, int, float]:
        snippet = self._text[start:end]
        matcher = SequenceMatcher(None, original, snippet)
        blocks = [block for block in matcher.get_matching_blocks() if block.size]
        if not blocks:
            score = self._similarity(original, snippet)
            return start, end, score

        refined_start = start + blocks[0].b
        refined_end = start + blocks[-1].b + blocks[-1].size
        padding = max(2, int(len(original) * 0.1))
        refined_start = max(start, refined_start - padding)
        refined_end = min(end, refined_end + padding)

        candidate = self._text[refined_start:refined_end]
        score = self._similarity(original, candidate)
        return refined_start, refined_end, score


def apply_fine_grained_edits(
    current_solution: str,
    changes_list: Iterable[Any],
    section_number_map: dict[int, str] | None = None,
) -> FineGrainedEditResult:
    """
    åœ¨ç”±section_idæ ‡è¯†çš„ç« èŠ‚å†…åº”ç”¨å¥å­çº§ç»†ç²’åº¦è¡¥ä¸ã€‚

    Args:
        current_solution: å½“å‰æ–‡æ¡£å†…å®¹
        changes_list: è¡¥ä¸åˆ—è¡¨
        section_number_map: æ•°å­—ç¼–å·â†’UUIDæ˜ å°„è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ”¯æŒç®€å•æ•°å­—å¼•ç”¨ï¼‰
    """
    # æ£€æŸ¥æ–‡æ¡£å¤§å°ï¼Œé¿å…å¤„ç†è¶…å¤§æ–‡æ¡£æ—¶æ€§èƒ½é—®é¢˜
    if len(current_solution) > 1_000_000:  # è¶…è¿‡1MB
        logging.warning(f"âš ï¸  æ–‡æ¡£è¿‡å¤§ ({len(current_solution):,} å­—ç¬¦ â‰ˆ {len(current_solution) // 1024}KB)ï¼Œå¯èƒ½å¤„ç†è¾ƒæ…¢\nğŸ’¡ å»ºè®®: è€ƒè™‘åˆ†æ®µå¤„ç†æˆ–ä¼˜åŒ–æ–‡æ¡£ç»“æ„")
    # noqa: W293
    modified_solution = current_solution
    changes = list(changes_list)
    logging.info("--- å¼€å§‹åº”ç”¨ %s ä¸ªç« èŠ‚çš„ç»†ç²’åº¦ä¿®è®¢ ---", len(changes))
    if section_number_map:
        logging.debug("  - æ•°å­—æ˜ å°„è¡¨ï¼š%s", {k: v[:8] + "..." for k, v in section_number_map.items()})
    total_successful_edits = 0
    total_failed_edits: list[str] = []
    sections_with_changes = 0
    total_replacements_applied = 0

    def _as_dict(change_item: Any) -> JsonDict:
        if isinstance(change_item, Mapping):
            return _normalize_mapping(change_item)
        try:
            dumped: Any = change_item.model_dump(mode="json")  # type: ignore[attr-defined]
        except AttributeError:
            try:
                dumped = dict(change_item)
            except (TypeError, ValueError):
                dumped = None
        if isinstance(dumped, Mapping):
            return _normalize_mapping(dumped)
        logging.warning("æ— æ³•è§£æè¡¥ä¸å˜æ›´é¡¹ï¼Œå·²è·³è¿‡ï¼š%s", change_item)
        return {}

    threshold_env = os.getenv("PATCH_SIMILARITY_THRESHOLD", "0.60")  # é™ä½é»˜è®¤ç¯å¢ƒå˜é‡é˜ˆå€¼
    try:
        similarity_threshold = float(threshold_env)
        if similarity_threshold <= 0 or similarity_threshold > 1:
            raise ValueError
    except ValueError:
        similarity_threshold = 0.60  # ä½¿ç”¨æ›´ä½çš„å›é€€é˜ˆå€¼

    for change in changes:
        change_dict = _as_dict(change)
        raw_target_id = change_dict.get("target_id")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¥‡ ä¼˜å…ˆçº§ 1ï¼šæ•°å­—ç¼–å· â†’ UUID æ˜ å°„ï¼ˆæœ€æ¨èï¼‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if isinstance(raw_target_id, int):
            if section_number_map and raw_target_id in section_number_map:
                target_id = section_number_map[raw_target_id]
                logging.debug("  âœ“ æ•°å­—ç¼–å· [%d] â†’ UUID %s...", raw_target_id, target_id[:8] if len(target_id) >= 8 else target_id)
            else:
                valid_range = f"1-{len(section_number_map)}" if section_number_map else "æ— æ˜ å°„è¡¨"
                logging.warning("  âœ— æ— æ•ˆçš„æ•°å­—ç¼–å· %dï¼ˆæœ‰æ•ˆèŒƒå›´:      %sï¼‰ï¼Œè·³è¿‡æ­¤è¡¥ä¸", raw_target_id, valid_range)
                continue
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¥ˆ ä¼˜å…ˆçº§ 2ï¼šå­—ç¬¦ä¸²ï¼ˆUUID æˆ–å…¶ä»–ï¼‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        else:
            target_id = str(raw_target_id or "")

        edits_value = change_dict.get("edits", [])
        if not isinstance(edits_value, list):
            logging.debug("  - ç« èŠ‚ '%s' çš„ edits å­—æ®µä¸æ˜¯åˆ—è¡¨ï¼Œè·³è¿‡ã€‚", target_id or "<missing>")
            continue
        edits_raw: list[JsonDict] = [_normalize_mapping(edit_map) for edit_map in cast(list[Any], edits_value)]
        if not target_id:
            logging.warning("  - é‡åˆ°ç¼ºå°‘ target_id çš„è¡¥ä¸ï¼Œå·²è·³è¿‡ã€‚")
            continue
        if not edits_raw:
            logging.info("  - ç« èŠ‚ '%s' æ— éœ€ä¿®è®¢ï¼Œè·³è¿‡ã€‚", target_id)
            continue

        original_sentences_for_fallback = [str(edit_dict.get("original_sentence", "")).strip() for edit_dict in edits_raw if str(edit_dict.get("original_sentence", "")).strip()]

        # å°è¯•ä»å˜æ›´å­—å…¸ä¸­æå–ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        section_title = change_dict.get("section_title") or change_dict.get("title")

        # ä½¿ç”¨å¢å¼ºçš„å¤šæ–¹å¼åŒ¹é…
        match, match_method = _find_section_by_multiple_methods(modified_solution, target_id, section_title)

        # å¦‚æœå¤šæ–¹å¼åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŸºäºåŸå¥å†…å®¹çš„å›é€€åŒ¹é…
        if not match and original_sentences_for_fallback:
            fallback_match = _find_section_by_sentences(modified_solution, original_sentences_for_fallback)
            if fallback_match:
                logging.warning("  - æœªèƒ½é€šè¿‡ID '%s' å®šä½ç« èŠ‚ï¼Œå·²åŸºäºåŸå¥å†…å®¹å›é€€åŒ¹é…ã€‚", target_id)
                match = fallback_match
                match_method = "sentences"

        if not match:
            logging.warning("  - æœªèƒ½æ‰¾åˆ°IDä¸º '%s' çš„ç« èŠ‚å—ï¼Œè·³è¿‡æ­¤ä¿®è®¢ã€‚", target_id)
            total_failed_edits.append(target_id)
            continue

        # è®°å½•æˆåŠŸçš„åŒ¹é…æ–¹å¼
        if match_method:
            logging.info(f"  - ç« èŠ‚ '{target_id}' é€šè¿‡ {match_method} æ–¹å¼åŒ¹é…æˆåŠŸ")

        original_section_content = match.group(1)
        section_lines = original_section_content.splitlines()
        heading_line = section_lines[0] if section_lines else ""
        body_text = "\n".join(section_lines[1:]) if len(section_lines) > 1 else ""

        corrector = EditCorrector(body_text, similarity_threshold=similarity_threshold)
        applied_count = 0
        replacement_total = 0
        section_failures: list[str] = []

        original_sentence = ""
        revised_sentence = ""
        for edit_dict in edits_raw:
            original_sentence = str(edit_dict.get("original_sentence", ""))
            revised_sentence = str(edit_dict.get("revised_sentence", ""))

            if not original_sentence.strip() or not revised_sentence.strip():
                logging.warning(
                    "  - ç« èŠ‚ '%s' ä¿®è®¢è·³è¿‡ï¼šå­˜åœ¨ç©ºçš„åŸå¥æˆ–ä¿®è®¢å¥ã€‚",
                    target_id,
                )
                continue

            if original_sentence.lstrip().startswith("#") or "section_id:" in original_sentence:
                logging.debug("  - è·³è¿‡è§¦åŠæ ‡é¢˜/ID çš„ä¿®è®¢ã€‚")
                continue

            metadata_obj = edit_dict.get("metadata")
            metadata: dict[str, Any] | None
            if isinstance(metadata_obj, Mapping):
                metadata = _normalize_mapping(metadata_obj)
            else:
                metadata = None
            expected_source = metadata.get("expected_replacements") if metadata else edit_dict.get("expected_replacements", 1)
            expected_raw: Any = expected_source
            try:
                expected_int = int(expected_raw)
            except (TypeError, ValueError):
                expected_int = 1
            if expected_int <= 0:
                expected_int = 1

            intent = EditIntent(
                original_sentence=original_sentence,
                revised_sentence=revised_sentence,
                expected_replacements=expected_int,
                metadata=metadata,
            )
            outcome = corrector.apply(intent)
            if outcome.applied:
                if expected_int > 0 and outcome.replacements != expected_int:
                    logging.warning(
                        "  - ç« èŠ‚ '%s' ä¿®è®¢æ›¿æ¢æ¬¡æ•°ä¸ç¬¦ (expected=%s, actual=%s)ã€‚å°†ç»§ç»­åº”ç”¨æ­¤ä¿®è®¢ã€‚",
                        target_id,
                        expected_int,
                        outcome.replacements,
                    )
                applied_count += 1
                replacement_total += max(1, outcome.replacements)
                if outcome.similarity is not None:
                    logging.info(
                        "    Â· ä¿®è®¢å‘½ä¸­ (%s Â· replacements=%s Â· similarity=%.2f)",
                        outcome.method,
                        outcome.replacements or 1,
                        outcome.similarity,
                    )
                else:
                    logging.info(
                        "    Â· ä¿®è®¢å‘½ä¸­ (%s Â· replacements=%s)",
                        outcome.method,
                        outcome.replacements or 1,
                    )
            else:
                # ç®€åŒ–æ—¥å¿—ï¼šè¯¦ç»†ä¿¡æ¯æ”¾åˆ° DEBUG çº§åˆ«
                logging.debug(
                    "  - ç« èŠ‚ '%s' ä¿®è®¢æœªå‘½ä¸­: %s åŸå¥ç‰‡æ®µ: '%s...'",
                    target_id,
                    outcome.detail,
                    original_sentence[:50],
                )
                section_failures.append(f"{target_id}: {outcome.detail} ({original_sentence[:30]}...)")

        updated_body = corrector.text
        if applied_count > 0 and updated_body != body_text:
            new_section_content = heading_line
            if updated_body:
                new_section_content += "\n" + updated_body
            modified_solution = modified_solution.replace(original_section_content, new_section_content, 1)
            logging.info(
                "  - æˆåŠŸå‘ç« èŠ‚ '%s' åº”ç”¨äº† %s/%s æ¡å¥å­çº§ä¿®è®¢ï¼ˆå®é™…æ›¿æ¢ %s æ¬¡ï¼‰ã€‚",
                target_id,
                applied_count,
                len(edits_raw),
                replacement_total,
            )
            sections_with_changes += 1
            total_replacements_applied += replacement_total
        else:
            logging.info(
                "  - ç« èŠ‚ '%s' æ— å®é™…å†…å®¹å˜æ›´ï¼ˆå‘½ä¸­ %s æ¡ä¿®è®¢ï¼‰ã€‚",
                target_id,
                applied_count,
            )

        if section_failures:
            total_failed_edits.extend(section_failures)
        if applied_count:
            total_successful_edits += applied_count

    logging.info("--- æ‰€æœ‰ç»†ç²’åº¦ä¿®è®¢åº”ç”¨å®Œæ¯• ---")
    if total_successful_edits:
        logging.info("  Â· æˆåŠŸå‘½ä¸­çš„ä¿®è®¢æ¡æ•°ï¼š%s", total_successful_edits)
    if total_failed_edits:
        # ç®€åŒ–æ—¥å¿—ï¼šåªæ˜¾ç¤ºæœªå‘½ä¸­æ•°é‡ï¼Œè¯¦ç»†åŸå› åœ¨ DEBUG
        logging.info("  Â· æœªå‘½ä¸­çš„ä¿®è®¢æ¡æ•°ï¼š%s", len(total_failed_edits))
        logging.debug("  Â· æœªå‘½ä¸­è¯¦æƒ…ï¼š%s", "; ".join(total_failed_edits))
    return FineGrainedEditResult(
        updated_text=modified_solution,
        sections_modified=sections_with_changes,
        successful_edits=total_successful_edits,
        failed_edits=total_failed_edits,
        total_replacements=total_replacements_applied,
    )
