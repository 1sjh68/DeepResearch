# =============================================================================
# DeepResearch 配置文件示例
# =============================================================================
# 使用说明:
#   1. 复制此文件: copy env.example env (Windows) 或 cp env.example env (Linux/Mac)
#   2. 填入你的实际 API 密钥和配置
#   3. 根据需求调整可选参数
# =============================================================================

# -----------------------------------------------------------------------------
# 核心 API 密钥与端点配置 (Core API Keys & Endpoints)
# -----------------------------------------------------------------------------
# [必需] DeepSeek API 密钥。
# DeepSeek 主写作/推理模型的访问密钥。
# 获取地址: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# [必需] 嵌入模型 (Embedding Model) 的 API 密钥。
# 生成向量嵌入与向量库交互所需的密钥。
# 如使用 Gitee AI: https://ai.gitee.com/
EMBEDDING_API_KEY=your_embedding_api_key_here

# [推荐] Google 自定义搜索 API 密钥 (JSON 数组格式)。
# 外部研究调用 Google Custom Search 的密钥列表。
# 注意：pydantic-settings 要求 list 类型使用 JSON 数组格式
# 获取地址: https://console.cloud.google.com/apis/credentials
GOOGLE_API_KEYS=["your_google_api_key_1","your_google_api_key_2"]

# [推荐] Google 自定义搜索引擎 ID (必须与 API 密钥一一对应)。
# 与 GOOGLE_API_KEYS 一一对应的搜索引擎 ID。
# 获取地址: https://programmablesearchengine.google.com/
GOOGLE_CSE_IDS=["your_cse_id_1","your_cse_id_2"]

# [可选] DeepSeek API 的基础 URL。
# 官方支持的URL: https://api.deepseek.com 或 https://api.deepseek.com/v1
# DeepSeek REST 接口地址，通常无需修改。
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# [可选] 嵌入模型的 API 基础 URL。
# 嵌入模型服务端点，如更换供应商需调整。
# Gitee AI: https://ai.gitee.com/v1
EMBEDDING_API_BASE_URL=https://ai.gitee.com/v1

# -----------------------------------------------------------------------------
# 核心任务定义 (Core Task Definition)
# -----------------------------------------------------------------------------
# [必需] 核心写作任务。由于 .env 文件不支持多行字符串，
# 复杂的长文本建议放在单独的文件中，然后通过 EXTERNAL_FILES 引用。
# 示例: "撰写一篇关于人工智能发展趋势的综述文章"
USER_PROBLEM=请在此处填写你的写作任务描述

# [可选] 本地参考文件或目录路径。
# 支持多个路径，使用逗号分隔；Windows 下建议使用正斜杠或用双引号包裹含空格/中文的路径，例如：
# EXTERNAL_FILES="./data/ref, \"./含空格目录\""
# 如果指定的是目录，系统会递归读取其中受支持的文件类型（.txt/.md/.pdf/.docx/.pptx 等）。
# 注意：pydantic-settings 会自动去除外层引号，直接写路径即可
# 建议使用相对路径，或者根据实际情况修改为您的参考文件目录
# EXTERNAL_FILES=./data/ref
EXTERNAL_FILES=

# -----------------------------------------------------------------------------
# AI 模型角色分配 (AI Model Role Assignment)
# -----------------------------------------------------------------------------
# 推荐使用 deepseek-reasoner 进行核心的理论推导和分析。
# 初稿生成使用的主要模型。
MAIN_AI_MODEL=deepseek-chat
# 重度推理或高成本场景的备用模型。
MAIN_AI_MODEL_HEAVY=deepseek-reasoner
# 评审、二次检查等辅助模型。
SECONDARY_AI_MODEL=deepseek-chat
# 推荐使用 deepseek-coder 进行结构化任务，如打补丁和生成大纲。
# 补丁生成与结构化输出模型。
PATCHER_MODEL_NAME=deepseek-coder
# 章节/材料摘要模型。
SUMMARY_MODEL_NAME=deepseek-coder
# 初始文档大纲生成模型。
OUTLINE_MODEL_NAME=deepseek-coder
# 大纲审查与校正模型。
PLANNING_REVIEW_MODEL_NAME=deepseek-coder
# JSON 修复模型，用于纠正格式错误的响应。
JSON_FIXER_MODEL_NAME=deepseek-coder
# 推荐使用 deepseek-chat 进行需要一定创造性和语言流畅度的任务。
# 外部研究摘要模型。
RESEARCHER_MODEL_NAME=deepseek-chat
# 文本润色模型。
EDITORIAL_MODEL_NAME=deepseek-chat
# 嵌入模型名称
# 向量数据库使用的嵌入模型标识。
EMBEDDING_MODEL_NAME=bge-m3

# -----------------------------------------------------------------------------
# [优化后] 核心运行参数
# -----------------------------------------------------------------------------
# 对于复杂的物理问题，增加迭代次数有助于AI进行更深入的自我修正。
# 最大迭代轮数（评审-补丁循环次数）。
MAX_ITERATIONS=2
# 目标字符数
# 初稿目标字数，用于分配章节篇幅。
INITIAL_SOLUTION_TARGET_CHARS=5000

# -----------------------------------------------------------------------------
# [优化后] RAG 与向量数据库参数
# -----------------------------------------------------------------------------
# 向量数据库根目录，用于存储检索经验。
# 使用相对路径，数据库将创建在项目根目录下
VECTOR_DB_PATH=./chroma_db
# 默认集合名称，不同任务共享此集合。
VECTOR_DB_COLLECTION_NAME="experience_store"
# 确保不超过Gitee API的上限。
# 每批发送到嵌入服务的文本数量。
EMBEDDING_BATCH_SIZE=25
# 增加从长期经验库中检索的数量，为AI提供更丰富的历史参考。
# 每次从经验库读取的最大条目数。
NUM_RETRIEVED_EXPERIENCES=5

# -----------------------------------------------------------------------------
# [优化后] LLM 调用微调参数
# -----------------------------------------------------------------------------
# 保持较低的温度以确保理论推导的严谨性和准确性。
# 事实/推理任务采样温度，建议保持较低。
LLM_TEMPERATURE_FACTUAL=0.1
# 润色和风格生成时允许稍高的创造性，但对于学术报告不宜过高。
# 创造性任务采样温度，值越高输出越发散。
LLM_TEMPERATURE_CREATIVE=0.2
# 略微降低频率惩罚，因为在技术文档中，关键术语的重复是必要的。
# 惩罚重复词汇的强度。
LLM_FREQUENCY_PENALTY=0.2
# 略微增加存在惩罚，鼓励AI在段落内部探索不同的表述方式，避免单调。
# 鼓励引入新主题的惩罚系数。
LLM_PRESENCE_PENALTY=0.1

# -----------------------------------------------------------------------------
# 开发者与高级参数 (保持默认)
# -----------------------------------------------------------------------------
# 是否启用交互模式（True 时会等待用户确认继续）。
INTERACTIVE_MODE=False
# 是否启用异步研究流水线（当前实现未使用，建议保持 False）。
USE_ASYNC_RESEARCH=False
# 是否允许调用外部网络研究功能。
ENABLE_WEB_RESEARCH=False
# 是否自动修正生成过程中的大纲结构。
ENABLE_DYNAMIC_OUTLINE_CORRECTION=True
# 单次 API 请求的最大等待时间（秒）。
API_REQUEST_TIMEOUT_SECONDS=100
# 重试等待时间的倍率，用于指数退避。
API_RETRY_WAIT_MULTIPLIER=2
# 单次重试等待的最大秒数。
API_MAX_RETRY_WAIT_SECONDS=60
# 文本分块时允许的最大 token 数。
MAX_CHUNK_TOKENS=4096
# 分块之间的字符重叠长度，保证上下文连续。
OVERLAP_CHARS=800
# 每个章节允许的最大分块数量。
MAX_CHUNKS_PER_SECTION=40
# 外部搜索每次返回的结果数量。
NUM_SEARCH_RESULTS=4
# 针对单个知识缺口最多尝试的查询次数。
MAX_QUERIES_PER_GAP=2
# 评审节点允许的最大上下文 token 数（字符串格式保持兼容性）。
MAX_CONTEXT_TOKENS_REVIEW="8000"

# 针对 LLM 客户端的专用代理开关（如需禁用代理，请显式设置为 true）
# LLM_DISABLE_PROXY=true

# -----------------------------------------------------------------------------
# 测试与调试参数
# -----------------------------------------------------------------------------
# 是否禁用最终质量检查（测试时可开启以节省时间和成本）
# DISABLE_FINAL_QUALITY_CHECK=true

# 是否启用详细日志输出（开发调试时使用）
# LOG_LEVEL=DEBUG

# 是否使用简单运行器（线性执行模式，推荐）
# USE_SIMPLE_RUNNER=true
