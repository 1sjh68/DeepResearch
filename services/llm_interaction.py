# services/llm_interaction.py
"""
LLM API äº¤äº’æ¨¡å—ï¼ˆé‡æ„ç‰ˆï¼‰

æä¾›æ ¸å¿ƒçš„AIè°ƒç”¨æ¥å£ï¼Œå·²æå–ä»¥ä¸‹åŠŸèƒ½åˆ°ç‹¬ç«‹æ¨¡å—ï¼š
- JSONä¿®å¤ â†’ utils.json_repair
- æ¶ˆæ¯å¤„ç† â†’ services.llm.message_processor
- é‡è¯•ç­–ç•¥ â†’ services.llm.retry_strategy
"""

import json
import logging
import re
import time
import types
from collections.abc import Mapping
from typing import Any, Union, cast, get_args, get_origin

import openai

# å…ˆå®šä¹‰ logger
logger = logging.getLogger(__name__)


def _categorize_error(e: Exception) -> str:
    """åˆ†ç±»é”™è¯¯ç±»å‹ä»¥ç¡®å®šé‡è¯•ç­–ç•¥

    Returns:
        str: é”™è¯¯ç±»åˆ« - 'instructor_retry', 'rate_limit', 'network', 'validation', 'unknown'
    """
    error_str = str(e).lower()
    error_type_name = type(e).__name__

    # InstructorRetryException
    if InstructorRetryException is not None and isinstance(e, InstructorRetryException):
        return "instructor_retry"

    # é€Ÿç‡é™åˆ¶é”™è¯¯
    if any(keyword in error_str for keyword in ["rate limit", "429", "too many requests", "quota exceeded"]):
        return "rate_limit"
    if hasattr(e, "status_code") and getattr(e, "status_code", None) == 429:
        return "rate_limit"

    # ç½‘ç»œé”™è¯¯
    if any(keyword in error_str for keyword in ["timeout", "connection", "network", "ssl", "certificate"]):
        return "network"
    if "RemoteProtocolError" in error_type_name or "ConnectError" in error_type_name:
        return "network"

    # éªŒè¯é”™è¯¯
    if "ValidationError" in error_type_name or "validation" in error_str:
        return "validation"

    # é»˜è®¤
    return "unknown"


# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from services.llm.message_processor import (  # noqa: E402
    clean_text_artifacts,
    coerce_message_content,
    ensure_json_instruction,
)
from services.llm.retry_strategy import (  # noqa: E402
    EmptyResponseFromReasonerError,
    build_retry_exception_types,
    build_retryer,
)
from utils.json_repair import (  # noqa: E402
    massage_structured_payload,
    repair_json_once,
)

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import instructor
    from instructor.exceptions import InstructorRetryException
    from pydantic import BaseModel as PydanticBaseModel

    instructor_available = True
except ImportError:  # pragma: no cover - optional dependency
    instructor_available = False
    instructor = None  # type: ignore
    InstructorRetryException = None  # type: ignore
    PydanticBaseModel = None  # type: ignore

# ä»é‡æ„åçš„æ¨¡å—ä¸­å¯¼å…¥ä¾èµ–
from config import Config  # noqa: E402
from config.constants import ModelLimits  # noqa: E402
from utils.progress_tracker import get_tracker  # noqa: E402


def _default_frequency_penalty(config: Config) -> float:
    if hasattr(config, "generation") and getattr(config.generation, "frequency_penalty", None) is not None:
        return config.generation.frequency_penalty
    return getattr(config, "frequency_penalty", 0.0)


def _default_presence_penalty(config: Config) -> float:
    if hasattr(config, "generation") and getattr(config.generation, "presence_penalty", None) is not None:
        return config.generation.presence_penalty
    return getattr(config, "presence_penalty", 0.0)


def _ensure_sync_client(config: Config) -> openai.OpenAI:
    """
    å»¶è¿Ÿåˆå§‹åŒ–å¹¶è¿”å›åŒæ­¥çš„DeepSeekå®¢æˆ·ç«¯ã€‚
    ç»Ÿä¸€çš„è¾…åŠ©å‡½æ•°ï¼Œé¿å…é‡å¤çš„å›é€€é€»è¾‘ã€‚
    """
    client_obj = config.client
    if client_obj is None:
        logging.info("åŒæ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ­£åœ¨å³æ—¶åˆå§‹åŒ–...")
        config.initialize_deepseek_client()
        client_obj = config.client
    if client_obj is None:
        raise RuntimeError("DeepSeek å®¢æˆ·ç«¯æœªèƒ½åˆå§‹åŒ–ã€‚")
    return client_obj


def _build_chat_call_params(
    model_name: str,
    messages: list[dict[str, str]],
    max_tokens: int,
    temperature: float,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
) -> tuple[dict[str, Any], bool]:
    """
    å‡†å¤‡èŠå¤©è¡¥å…¨è°ƒç”¨å‚æ•°ï¼ŒåŒæ—¶å¤„ç†æ¨ç†å™¨ç‰¹å®šçš„çº¦æŸã€‚
    è¿”å›(<call_params>, <is_reasoner_model>)ã€‚
    """
    is_reasoner_model = "reasoner" in model_name.lower()
    call_params: dict[str, Any] = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "stream": False,
    }
    if not is_reasoner_model:
        call_params.update({
            "temperature": temperature,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
        })
    return call_params, is_reasoner_model


# ä»¥ä¸‹å‡½æ•°å·²ç§»è‡³ services.llm.message_processor å’Œ utils.json_repair
# æ­¤å¤„ä¿ç•™åˆ«åä»¥ä¿æŒå‘åå…¼å®¹
_ensure_json_instruction = ensure_json_instruction
_coerce_message_content = coerce_message_content
_clean_text_artifacts = clean_text_artifacts


# ä»¥ä¸‹JSONå¤„ç†å‡½æ•°å·²ç§»è‡³ utils/json_repairï¼Œæ­¤å¤„ä¿ç•™åˆ«å
from utils.json_repair import (  # noqa: E402
    _safe_model_validate,  # å†…éƒ¨ä½¿ç”¨
)

_massage_structured_payload = massage_structured_payload  # ä¿æŒå‘åå…¼å®¹


# repair_json_once ç°åœ¨ä» utils.json_repair å¯¼å…¥ï¼Œæ”¯æŒdebugå‚æ•°
def _repair_json_once_compat(text: str, schema: type[Any]) -> tuple[str, bool]:
    """å…¼å®¹æ€§åŒ…è£…ï¼Œè°ƒç”¨æ–°çš„repair_json_once"""
    # å°è¯•ä»configè·å–debugè®¾ç½®
    try:
        from config.config import Config

        debug = getattr(Config().workflow, "debug_json_repair", False)
    except Exception:
        debug = False
    return repair_json_once(text, schema, debug=debug)


# è¿™ä¸ªå—æ ‡è®°ç”¨äºä¸‹ä¸€æ­¥åˆ é™¤
def call_ai_with_schema(
    config: Config,
    model_name: str,
    messages: list[dict[str, Any]],
    schema: type[Any],
    kwargs: dict[str, Any],
) -> tuple[Any | str, str]:
    """ä½¿ç”¨ Pydantic schema è¿›è¡Œç»“æ„åŒ–è°ƒç”¨çš„å¢å¼ºç‰ˆæœ¬ã€‚"""
    if not instructor_available:
        logging.warning("Instructorä¸å¯ç”¨ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨")
        content = call_ai(
            config,
            model_name,
            messages,
            temperature=kwargs.get("temperature"),
            max_tokens_output=kwargs.get("max_tokens_output", -1),
            top_p=kwargs.get("top_p"),
            frequency_penalty=kwargs.get("frequency_penalty"),
            presence_penalty=kwargs.get("presence_penalty"),
            response_format=kwargs.get("response_format"),
            schema=kwargs.get("schema"),
        )
        return content, "instructor_unavailable"

    if PydanticBaseModel is not None:
        try:
            if not issubclass(schema, PydanticBaseModel):
                logging.warning(
                    "æä¾›çš„ schema %s ä¸æ˜¯ Pydantic BaseModel å­ç±»ï¼Œå¯èƒ½å¯¼è‡´ç»“æ„åŒ–è°ƒç”¨å¤±è´¥ã€‚",
                    schema,
                )
        except TypeError:
            logging.warning("æä¾›çš„ schema %s æ— æ³•ç”¨äº issubclass æ£€æŸ¥ã€‚", schema)

    try:
        # ä½¿ç”¨ instructor.patch åˆ›å»ºç»“æ„åŒ–å®¢æˆ·ç«¯
        # æ³¨æ„ï¼šæˆ‘ä»¬ç¦ç”¨äº† instructor çš„å†…éƒ¨é‡è¯•ï¼ˆmax_retries=0ï¼‰ï¼Œå› ä¸ºï¼š
        # 1. instructor é‡è¯•æ—¶ä¼šä¿®æ”¹ messagesï¼ˆæ·»åŠ å“åº”å†å²ï¼‰
        # 2. è¿™äº›ä¿®æ”¹ç»•è¿‡äº† _coerce_message_content çš„è§„èŒƒåŒ–
        # 3. å¤–å±‚çš„ tenacity é‡è¯•ä¼šé‡æ–°è°ƒç”¨æœ¬å‡½æ•°ï¼Œç¡®ä¿æ¶ˆæ¯å§‹ç»ˆè¢«è§„èŒƒåŒ–
        client = _ensure_sync_client(config)

        if instructor is None:
            raise RuntimeError("ç»“æ„åŒ–è°ƒç”¨å¤±è´¥ï¼šinstructor åº“ä¸å¯ç”¨ã€‚")

        structured_client = cast(Any, instructor).patch(client)

        # å‡†å¤‡è°ƒç”¨å‚æ•°
        normalized_messages = _coerce_message_content(messages)
        if logging.getLogger(__name__).isEnabledFor(logging.DEBUG):
            for idx, msg in enumerate(normalized_messages):
                content = msg.get("content")
                if not isinstance(content, str):
                    logging.debug(
                        "Message %s content type after coercion is %s (expected str)",
                        idx,
                        type(content).__name__,
                    )
                residual_keys = {"tool_calls", "function_call"} & msg.keys()
                if residual_keys:
                    logging.debug(
                        "Message %s retains structured fields post-coercion: %s",
                        idx,
                        {key: msg[key] for key in residual_keys},
                    )

        call_params: dict[str, Any] = {
            "model": model_name,
            "messages": normalized_messages,
            "response_model": cast(Any, schema),
        }

        # æ·»åŠ å…¶ä»–å‚æ•°ï¼Œå¤„ç†å‚æ•°åæ˜ å°„
        for k, v in kwargs.items():
            if k == "response_format":
                continue

            # ğŸ”§ å‚æ•°åæ˜ å°„ï¼šmax_tokens_output â†’ max_tokens
            if k == "max_tokens_output":
                # æ˜ å°„åˆ° max_tokensï¼Œä½†ä¸æ·»åŠ  max_tokens_output æœ¬èº«
                if isinstance(v, int) and v > 0 and "max_tokens" not in call_params:
                    call_params["max_tokens"] = v
                continue  # ç¡®ä¿ max_tokens_output ä¸è¢«ä¼ é€’åˆ° API

            # æ­£å¸¸æ·»åŠ å…¶ä»–å‚æ•°
            call_params[k] = v

        # ç¡®ä¿ max_tokens_output ä¸ä¼šè¢«æ„å¤–ä¼ é€’ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        call_params.pop("max_tokens_output", None)

        # ç¦ç”¨ instructor å†…éƒ¨é‡è¯•ï¼šinstructor åœ¨é‡è¯•æ—¶ä¼šå°†åŒ…å« tool_calls çš„å“åº”
        # è¿½åŠ åˆ° messagesï¼Œç»•è¿‡æˆ‘ä»¬çš„ _coerce_message_content è§„èŒƒåŒ–ï¼Œå¯¼è‡´ API 400 é”™è¯¯ã€‚
        # å¤–å±‚çš„ tenacity é‡è¯•æœºåˆ¶ä¼šåœ¨å¼‚å¸¸æ—¶é‡æ–°è°ƒç”¨æ•´ä¸ªå‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡éƒ½é‡æ–°è§„èŒƒåŒ–æ¶ˆæ¯ã€‚
        call_params["max_retries"] = 0

        response = structured_client.chat.completions.create(**call_params)
        logging.info(f"ç»“æ„åŒ–è°ƒç”¨æˆåŠŸï¼Œè¿”å› {type(response).__name__} ç±»å‹å¯¹è±¡")
        return response, "success"

    except Exception as e:
        error_msg = f"ç»“æ„åŒ–è°ƒç”¨å¤±è´¥: {e}"

        # å¢å¼ºçš„é”™è¯¯åˆ†ç±»å’Œå¤„ç†
        error_category = _categorize_error(e)

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¯ä¿®å¤çš„ValidationError
        is_repairable_error = False
        from pydantic import ValidationError

        # ç¬¬1æ­¥ï¼šä»å¼‚å¸¸ä¸­æå– ValidationErrorï¼ˆå¯èƒ½è¢« InstructorRetryException åŒ…è£…ï¼‰
        validation_error: ValidationError | None = None

        if isinstance(e, ValidationError):
            validation_error = e
            logging.debug("æ•è·åˆ°ç›´æ¥çš„ ValidationError")
        elif InstructorRetryException is not None and isinstance(e, InstructorRetryException):
            # InstructorRetryException å¯èƒ½åŒ…è£…äº† ValidationError
            # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„å±æ€§
            logging.debug("æ£€æµ‹åˆ° InstructorRetryExceptionï¼Œå°è¯•æå– ValidationError...")
            logging.debug("  - å¯ç”¨å±æ€§: %s", dir(e))

            if hasattr(e, "__cause__") and isinstance(e.__cause__, ValidationError):
                validation_error = e.__cause__
                logging.debug("âœ“ ä» InstructorRetryException.__cause__ æå–åˆ° ValidationError")
            elif hasattr(e, "last_exception") and isinstance(getattr(e, "last_exception", None), ValidationError):
                validation_error = e.last_exception
                logging.debug("âœ“ ä» InstructorRetryException.last_exception æå–åˆ° ValidationError")
            elif hasattr(e, "exception") and isinstance(getattr(e, "exception", None), ValidationError):
                validation_error = e.exception
                logging.debug("âœ“ ä» InstructorRetryException.exception æå–åˆ° ValidationError")
            else:
                # å°è¯•éå†å¼‚å¸¸é“¾
                logging.debug("  - å°è¯•éå†å¼‚å¸¸é“¾...")
                current = e
                max_depth = 5
                depth = 0
                while current and depth < max_depth:
                    depth += 1
                    if isinstance(current, ValidationError):
                        validation_error = current
                        logging.debug("âœ“ ä»å¼‚å¸¸é“¾æ·±åº¦ %d æå–åˆ° ValidationError", depth)
                        break
                    current = getattr(current, "__cause__", None)

                if not validation_error:
                    # å°è¯•æ›´å¤šæå–æ–¹æ³•
                    if hasattr(e, "args") and len(e.args) > 0:
                        for arg in e.args:
                            if isinstance(arg, ValidationError):
                                validation_error = arg
                                logging.debug("âœ“ ä» InstructorRetryException.args æå–åˆ° ValidationError")
                                break

                    if not validation_error:
                        logging.warning("âœ— æ— æ³•ä» InstructorRetryException æå– ValidationErrorï¼ˆå·²æ£€æŸ¥æ‰€æœ‰å¯èƒ½ä½ç½®ï¼‰")

        # ç¬¬2æ­¥ï¼šå¦‚æœæ‰¾åˆ°äº† ValidationErrorï¼Œå°è¯•æå–å¹¶ä¿®å¤æŸåçš„ JSON
        rescue_candidate: str | None = None

        if validation_error:
            error_str = str(validation_error)
            # æ‰©å±•å¯ä¿®å¤é”™è¯¯çš„åˆ¤æ–­æ¡ä»¶
            repairable_keywords = ["trailing characters", "json_invalid", "unterminated string", "expecting", "invalid escape", "unexpected character", "json.decoder.JSONDecodeError", "validation error"]
            if any(keyword in error_str.lower() for keyword in repairable_keywords):
                is_repairable_error = True
                # æš‚æ—¶ä¸è¾“å†ºERRORï¼Œå…ˆå°è¯•ä¿®å¤
                logging.debug("æ£€æµ‹åˆ°å¯ä¿®å¤çš„éªŒè¯é”™è¯¯ï¼ˆ%sï¼‰ï¼Œå°†å°è¯•ä½¿ç”¨ä¿®å¤å·¥å…·é“¾", "InstructorRetryExceptionåŒ…è£…" if isinstance(e, type(InstructorRetryException)) else "ç›´æ¥ValidationError")
            else:
                # å…¶ä»–éªŒè¯é”™è¯¯ä»ç„¶è¾“å‡ºä½†é™ä½æ—¥å¿—çº§åˆ«
                logging.warning(error_msg)  # ä» error æ”¹ä¸º warning

            # ä» ValidationError ä¸­æå–åŸå§‹ JSON æ–‡æœ¬
            try:
                errors_list = validation_error.errors()
                logging.debug("ValidationError.errors() è¿”å› %d ä¸ªé”™è¯¯é¡¹", len(errors_list))

                for idx, err_item in enumerate(errors_list):
                    logging.debug("  - é”™è¯¯ %d: type=%s, keys=%s", idx, err_item.get("type"), list(err_item.keys()))

                    # å°è¯•å¤šç§æå–æ–¹æ³•
                    raw_input = None
                    if "input" in err_item:
                        raw_input = err_item["input"]
                        logging.debug("    ä» err_item['input'] æå–")
                    elif "ctx" in err_item and isinstance(err_item["ctx"], dict):
                        raw_input = err_item["ctx"].get("input")
                        logging.debug("    ä» err_item['ctx']['input'] æå–")

                    if isinstance(raw_input, str) and raw_input.strip():
                        rescue_candidate = raw_input
                        logging.debug("âœ“ ä» ValidationError æå–åˆ°åŸå§‹ JSONï¼ˆé•¿åº¦=%sï¼Œå‰80å­—ç¬¦=%s...ï¼‰", len(raw_input), raw_input[:80].replace("\n", " "))
                        break
                    elif raw_input is not None:
                        logging.debug("    raw_input å­˜åœ¨ä½†ç±»å‹ä¸åŒ¹é…: %s", type(raw_input).__name__)

                if not rescue_candidate:
                    # æœ€åå°è¯•ï¼šä»é”™è¯¯æ¶ˆæ¯ä¸­æå– input_value
                    logging.debug("  - å°è¯•ä»é”™è¯¯å­—ç¬¦ä¸²ä¸­æå– input_value...")
                    if "input_value=" in error_str:
                        match = re.search(r"input_value='([^']+)'", error_str)
                        if match:
                            rescue_candidate = match.group(1)
                            # å¤„ç†è½¬ä¹‰
                            rescue_candidate = rescue_candidate.replace(r"\'", "'").replace(r"\"", '"')
                            logging.debug("âœ“ ä»é”™è¯¯æ¶ˆæ¯æå–åˆ° JSON ç‰‡æ®µï¼ˆé•¿åº¦=%sï¼‰", len(rescue_candidate))
            except Exception as err_extract:
                logging.debug("æ— æ³•ä» ValidationError ä¸­æå–åŸå§‹è¾“å…¥: %s", err_extract, exc_info=True)

            # ç¬¬3æ­¥ï¼šè°ƒç”¨å®Œæ•´çš„ä¿®å¤å·¥å…·é“¾ï¼ˆåŒ…æ‹¬ json-repair åº“ã€LaTeX å¤„ç†ç­‰ï¼‰
            if rescue_candidate:
                logging.debug("å¯åŠ¨ä¿®å¤å·¥å…·é“¾ï¼šjson-repairåº“ + LaTeXå¤„ç† + 7ç§å†…ç½®ç­–ç•¥")
                try:
                    # è·å– debug é…ç½®ï¼Œå¯ç”¨è¯¦ç»†çš„ä¿®å¤æ—¥å¿—
                    debug_mode = getattr(getattr(config, "workflow", None), "debug_json_repair", False)
                    repaired_text, repaired = repair_json_once(rescue_candidate, schema, debug=debug_mode)
                    candidate_text = repaired_text if repaired else rescue_candidate
                    candidate_text = _clean_text_artifacts(candidate_text)

                    data = json.loads(candidate_text)
                    parsed = _safe_model_validate(schema, _massage_structured_payload(schema, data))

                    if is_repairable_error:
                        logging.info("âœ“ ç»“æ„åŒ–è°ƒç”¨å¤±è´¥â€”â€”å·²é€šè¿‡ä¿®å¤å·¥å…·é“¾æ¢å¤ï¼ˆ%sï¼‰", "json-repair" if repaired else "å†…ç½®ç­–ç•¥")
                    else:
                        logging.info("ç»“æ„åŒ–è°ƒç”¨å¤±è´¥â€”â€”é€šè¿‡ä¿®å¤å·¥å…·é“¾æ¢å¤")
                    return parsed, "fallback_success"
                except json.JSONDecodeError as rescue_parse_error:
                    logging.debug("ä¿®å¤å·¥å…·é“¾ï¼šJSON è§£æå¤±è´¥: %s", rescue_parse_error)
                except Exception as rescue_validate_error:
                    logging.debug("ä¿®å¤å·¥å…·é“¾ï¼šæ¨¡å‹æ ¡éªŒå¤±è´¥: %s", rescue_validate_error)
        else:
            # é ValidationErrorï¼Œæ­£å¸¸è¾“å‡ºé”™è¯¯
            logging.error(error_msg)

        # è®°å½•è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ä»¥ä¾¿æ’æŸ¥
        if "BadRequestError" in type(e).__name__ or (hasattr(e, "status_code") and getattr(e, "status_code", None) == 400):
            logging.debug("BadRequestError è¯¦æƒ… - å¯èƒ½çš„æ¶ˆæ¯æ ¼å¼é—®é¢˜ã€‚è¯·æ£€æŸ¥ messages æ˜¯å¦åŒ…å«æœªåºåˆ—åŒ–çš„å¤æ‚å¯¹è±¡ï¼ˆå¦‚ tool_callsã€function_callï¼‰ã€‚")

        # ç¬¬4æ­¥ï¼šä¿®å¤å¤±è´¥åï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨ï¼ˆä¸ä½¿ç”¨ json_objectï¼‰
        logging.debug("ä¿®å¤å·¥å…·é“¾æœªèƒ½æ¢å¤ç»“æ„åŒ–è¾“å‡ºï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨ï¼ˆä¸ä½¿ç”¨ json_objectï¼‰")

        # ä¿å­˜é”™è¯¯æ ‡è®°åˆ°å‡½æ•°ä½œç”¨åŸŸå¤–
        _is_repairable_error = is_repairable_error
        plain_kwargs = {k: v for k, v in kwargs.items() if k != "response_format"}
        plain_kwargs["schema"] = None
        if "max_tokens" in plain_kwargs:
            max_tokens_value = plain_kwargs.pop("max_tokens")
            plain_kwargs["max_tokens_output"] = max_tokens_value

        json_retry_kwargs = plain_kwargs.copy()
        parsed_obj: Any | None = None
        force_plaintext = False
        structured_retry_count = 0
        max_structured_retry = 3  # å¢åŠ é‡è¯•æ¬¡æ•°

        # æ ¹æ®é”™è¯¯ç±»åˆ«å†³å®šé‡è¯•ç­–ç•¥
        if error_category == "instructor_retry":
            # InstructorRetryExceptionçš„æ™ºèƒ½å¤„ç†
            if rescue_candidate:
                # å·²ç»å°è¯•è¿‡ä¿®å¤ï¼Œä½†å¯èƒ½éœ€è¦æ”¹å˜ç­–ç•¥
                if structured_retry_count < max_structured_retry:
                    logging.info("æ­£åœ¨å°è¯•ç¬¬ %d/%d æ¬¡ç»“æ„åŒ–ä¿®å¤...", structured_retry_count + 1, max_structured_retry)
                    # ç»§ç»­å°è¯•ç»“æ„åŒ–é‡è¯•ï¼Œä½†ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å‚æ•°
                    json_retry_kwargs["temperature"] = min(0.7, json_retry_kwargs.get("temperature", 0.3) + 0.1)
                else:
                    logging.warning("InstructorRetryExceptionï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ã€‚")
                    force_plaintext = True
            elif validation_error:
                # æ‰¾åˆ°äº†ValidationErrorä½†æ— æ³•æå–JSONï¼Œå¯èƒ½éœ€è¦ç›´æ¥é‡è¯•
                if structured_retry_count < 1:  # è‡³å°‘è¯•ä¸€æ¬¡
                    logging.info("æ£€æµ‹åˆ°ValidationErrorï¼Œå°è¯•ç›´æ¥é‡æ–°è¯·æ±‚...")
                    # ä¸è®¾ç½®force_plaintextï¼Œç»§ç»­ç»“æ„åŒ–é‡è¯•
                else:
                    logging.warning("æ£€æµ‹åˆ° InstructorRetryException ä¸”æ‰¾åˆ° ValidationErrorï¼Œä½†æ— æ³•æå–æœ‰æ•ˆçš„ JSON inputï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æª€å¼ã€‚")
                    force_plaintext = True
            else:
                # æ—¢æ²¡æœ‰rescue_candidateä¹Ÿæ²¡æœ‰validation_errorï¼Œå¯èƒ½æ˜¯å…¶ä»–é—®é¢˜
                logging.warning("æ£€æµ‹åˆ° InstructorRetryException ä½†æ— æ³•ç¡®å®šå…·ä½“é—®é¢˜ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ã€‚")
                force_plaintext = True
        elif error_category == "rate_limit":
            # é€Ÿç‡é™åˆ¶ï¼Œåº”è¯¥ç­‰å¾…å¹¶é‡è¯•
            import random
            import time

            wait_time = random.uniform(2.0, 5.0)
            logging.warning(f"æ£€æµ‹åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
            time.sleep(wait_time)
            # ä¸è®¾ç½®force_plaintextï¼Œç»§ç»­é‡è¯•
        elif getattr(e, "status_code", None) == 404 or "404" in str(e):
            logging.warning("ç»“æ„åŒ–è°ƒç”¨è¿”å›404ï¼Œå¼ºåˆ¶ä½¿ç”¨çº¯æ–‡æœ¬é™çº§æ¨¡å¼ã€‚")
            force_plaintext = True

        def _attempt_structured_json(
            base_messages: list[dict[str, Any]],
            attempt_label: str,
        ) -> Any | None:
            nonlocal structured_retry_count
            if structured_retry_count >= max_structured_retry:
                logging.debug(
                    "call_ai_with_schema: è¾¾åˆ°ç»“æ„åŒ–é‡è¯•ä¸Šé™(%s)ï¼Œè·³è¿‡ %s",
                    max_structured_retry,
                    attempt_label,
                )
                return None
            structured_retry_count += 1
            attempt_messages = _coerce_message_content([dict(m) for m in base_messages])
            attempt_messages = _ensure_json_instruction(attempt_messages)
            attempt_messages = _coerce_message_content(attempt_messages)
            raw_response = call_ai(
                config,
                model_name,
                attempt_messages,
                temperature=json_retry_kwargs.get("temperature"),
                max_tokens_output=json_retry_kwargs.get("max_tokens_output", -1),
                top_p=json_retry_kwargs.get("top_p"),
                frequency_penalty=json_retry_kwargs.get("frequency_penalty"),
                presence_penalty=json_retry_kwargs.get("presence_penalty"),
                response_format={"type": "json_object"},
                schema=None,
            )
            if not isinstance(raw_response, str) or not raw_response or "AIæ¨¡å‹è°ƒç”¨å¤±è´¥" in raw_response:
                logging.debug(
                    "call_ai_with_schema: %s æ— æœ‰æ•ˆå­—ç¬¦ä¸²å“åº”ï¼Œraw_type=%s",
                    attempt_label,
                    type(raw_response).__name__,
                )
                return None

            logging.debug(
                "call_ai_with_schema: %s raw length=%s",
                attempt_label,
                len(raw_response),
            )
            debug_mode = getattr(getattr(config, "workflow", None), "debug_json_repair", False)
            repaired_text, repaired = repair_json_once(raw_response, schema, debug=debug_mode)
            logging.debug(
                "call_ai_with_schema: %s repair_result repaired=%s, length=%s",
                attempt_label,
                repaired,
                len(repaired_text) if isinstance(repaired_text, str) else "n/a",
            )
            candidate_text = repaired_text if repaired else raw_response
            candidate_text = _clean_text_artifacts(candidate_text)
            try:
                data = json.loads(candidate_text)
            except json.JSONDecodeError as parse_error:
                logging.debug(
                    "ç»“æ„åŒ–å›é€€è§£æå¤±è´¥ï¼ˆJSON è§£æï¼Œ%sï¼‰: %s",
                    attempt_label,
                    parse_error,
                )
            else:
                try:
                    parsed = _safe_model_validate(schema, _massage_structured_payload(schema, data))
                    # å¦‚æœä¹‹å‰æ˜¯å¯ä¿®å¤é”™è¯¯ä¸”ä¿®å¤æˆåŠŸï¼Œè¾“å‡ºç®€æ´ä¿¡æ¯
                    if _is_repairable_error:
                        logging.info("âœ“ ç»“æ„åŒ–è°ƒç”¨å¤±è´¥â€”â€”å·²ä¿®å¤")
                    else:
                        logging.info("ç»“æ„åŒ–å›é€€è§£ææˆåŠŸï¼ˆ%sï¼‰ã€‚", attempt_label)
                    return parsed
                except Exception as parse_error:
                    logging.debug(
                        "ç»“æ„åŒ–å›é€€è§£æå¤±è´¥ï¼ˆæ¨¡å‹æ ¡éªŒï¼Œ%sï¼‰: %s",
                        attempt_label,
                        parse_error,
                    )
            return None

        def _describe_annotation(annotation: Any) -> str:
            if annotation is None:
                return "any"
            origin = get_origin(annotation)
            if origin is None:
                if annotation in (str,):
                    return "string"
                if annotation in (int,):
                    return "integer"
                if annotation in (float,):
                    return "number"
                if annotation in (bool,):
                    return "boolean"
                if annotation in (list, tuple, set):
                    return "array"
                if annotation in (dict, Mapping):
                    return "object"
                return str(annotation)

            if origin in (list, tuple, set):
                args = get_args(annotation)
                inner = _describe_annotation(args[0]) if args else "value"
                return f"array<{inner}>"

            if origin in (dict, Mapping):
                return "object"

            if origin in (Union, types.UnionType):
                args = get_args(annotation)
                if not args:
                    return "any"
                parts: list[str] = []
                has_none = False
                for item in args:
                    if item is type(None):
                        has_none = True
                        continue
                    parts.append(_describe_annotation(item))
                desc = " | ".join(parts) if parts else "any"
                if has_none:
                    desc = f"{desc} | null" if parts else "null"
                return desc

            return str(annotation)

        def _build_schema_skeleton_text() -> str | None:
            if not hasattr(schema, "model_fields"):
                return None
            model_fields = getattr(schema, "model_fields")
            if not model_fields:
                return None

            lines: list[str] = []
            for name, field in model_fields.items():
                annotation = getattr(field, "annotation", None)
                type_desc = _describe_annotation(annotation)
                required = "å¿…å¡«" if getattr(field, "is_required", lambda: False)() else "å¯é€‰"
                lines.append(f'- "{name}": {required}, ç±»å‹ {type_desc}')

            if not lines:
                return None

            skeleton_lines = [
                "ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSON å¯¹è±¡ï¼Œä¸å¾—åŒ…å« Markdownã€ä»£ç å—æˆ–é¢å¤–è¯´æ˜ã€‚",
                "ä»…ä½¿ç”¨ä»¥ä¸‹å­—æ®µï¼ˆä¸å¾—æ–°å¢å­—æ®µï¼‰ï¼š",
                *lines,
                "æ•°å€¼å­—æ®µè¯·ä½¿ç”¨æ•°å­—ï¼Œå¸ƒå°”å€¼è¯·ä½¿ç”¨ true/falseã€‚",
            ]
            return "\n".join(skeleton_lines)

        if not force_plaintext:
            parsed_obj = _attempt_structured_json(
                [dict(m) for m in messages],
                "json_object å¼ºåˆ¶(åŸå§‹)",
            )
            if parsed_obj is not None:
                return parsed_obj, "fallback_success"

        skeleton_text = _build_schema_skeleton_text()
        if skeleton_text and not force_plaintext:
            logging.info("call_ai_with_schema: å°è¯•éª¨æ¶ç»“æ„åŒ–é‡è¯•ã€‚")
            skeleton_messages = [{"role": "system", "content": skeleton_text}]
            skeleton_messages.extend(dict(m) for m in messages)
            parsed_obj = _attempt_structured_json(
                skeleton_messages,
                "json_object å¼ºåˆ¶(éª¨æ¶)",
            )
            if parsed_obj is not None:
                return parsed_obj, "fallback_success"

        if force_plaintext:
            logging.info("call_ai_with_schema: å·²å¯åŠ¨çº¯æ–‡æœ¬é™çº§ï¼Œä¸å†å°è¯•ç»“æ„åŒ–å“åº”ã€‚")

        content = call_ai(
            config,
            model_name,
            messages,
            temperature=plain_kwargs.get("temperature"),
            max_tokens_output=plain_kwargs.get("max_tokens_output", -1),
            top_p=plain_kwargs.get("top_p"),
            frequency_penalty=plain_kwargs.get("frequency_penalty"),
            presence_penalty=plain_kwargs.get("presence_penalty"),
            response_format=None,
            schema=None,
        )

        # ç®€å•å°è¯•ç›´æ¥è§£æä¸º JSON -> schemaï¼›è‹¥å¤±è´¥ï¼Œäº¤ç”±è°ƒç”¨æ–¹å¤„ç†
        json_text: str | None = None
        if content and "AIæ¨¡å‹è°ƒç”¨å¤±è´¥" not in content:
            try:
                from utils.text_processor import extract_json_from_ai_response
            except Exception as import_exc:  # pragma: no cover - defensive
                logging.debug("æ— æ³•å¯¼å…¥ JSON æå–å·¥å…·: %s", import_exc)
                extract_json_from_ai_response = None  # type: ignore
            logging.debug("call_ai_with_schema: fallback content length=%s", len(content))
            debug_mode = getattr(getattr(config, "workflow", None), "debug_json_repair", False)
            repaired_text, repaired = repair_json_once(content, schema, debug=debug_mode)
            logging.debug(
                "call_ai_with_schema: fallback repair result repaired=%s, length=%s",
                repaired,
                len(repaired_text) if isinstance(repaired_text, str) else "n/a",
            )
            if repaired:
                json_text = repaired_text
            if not json_text and extract_json_from_ai_response:
                json_text = extract_json_from_ai_response(
                    config,
                    content,
                    context_for_error_log=f"{model_name} fallback for {getattr(schema, '__name__', 'schema')}",
                )
            if json_text:
                json_text = _clean_text_artifacts(json_text)
                try:
                    data = json.loads(json_text)
                except json.JSONDecodeError as parse_error:
                    logging.warning(
                        "å›é€€ JSON æ— æ³•è§£æ %s: %s | ç‰‡æ®µ=%s",
                        getattr(schema, "__name__", "schema"),
                        parse_error,
                        json_text[:200].replace("\n", " ") if isinstance(json_text, str) else "<non-str>",
                    )
                else:
                    try:
                        parsed_obj = _safe_model_validate(schema, _massage_structured_payload(schema, data))
                        logging.info("ç»“æ„åŒ–å›é€€è§£ææˆåŠŸã€‚")
                        return parsed_obj, "fallback_success"
                    except Exception as parse_error:
                        logging.warning(
                            "å›é€€ JSON æ— æ³•æ„å»º %s: %s | ç‰‡æ®µ=%s",
                            getattr(schema, "__name__", "schema"),
                            parse_error,
                            json_text[:200].replace("\n", " ") if isinstance(json_text, str) else "<non-str>",
                        )
            else:
                logging.debug("å›é€€å“åº”ä¸­æœªèƒ½æå–åˆ°æœ‰æ•ˆ JSONã€‚")
        logging.warning(
            "ç»“æ„åŒ–å›é€€å¤±è´¥ï¼šschema=%sï¼Œcontent_len=%sï¼Œjson_text=%s",
            getattr(schema, "__name__", "schema"),
            len(content) if isinstance(content, str) else "n/a",
            "available" if json_text else "missing",
        )
        return content, "fallback_failed"


def call_ai_core(
    config: Config,
    model_name: str,
    messages: list[dict[str, str]],
    temperature: float,
    effective_max_output_tokens: int,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    response_format: dict[str, Any] | None = None,
) -> str:
    """
    æ ¸å¿ƒ AI è°ƒç”¨é€»è¾‘ (åŒæ­¥ç‰ˆæœ¬)ï¼Œç”± tenacity åŒ…è£…ä»¥å®ç°é‡è¯•ã€‚
    """
    client = _ensure_sync_client(config)

    start_time = time.perf_counter()
    # Emit a progress pulse to indicate the API request is being sent
    try:
        tracker = get_tracker(config.task_id)
        if tracker:
            tracker.pulse(f"è°ƒç”¨æ¨¡å‹ {model_name} ä¸­...ï¼ˆå‡†å¤‡å‘é€è¯·æ±‚ï¼‰")
    except Exception as e:
        logger.debug(f"è¿›åº¦è¿½è¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    call_params, is_reasoner_model = _build_chat_call_params(
        model_name,
        messages,
        effective_max_output_tokens,
        temperature,
        top_p,
        frequency_penalty,
        presence_penalty,
    )

    if response_format is not None:
        call_params["response_format"] = response_format

    response = client.chat.completions.create(**call_params)
    duration = time.perf_counter() - start_time

    message = response.choices[0].message
    content = message.content
    reasoning_content = getattr(message, "reasoning_content", None)

    total_tokens: int | None = None
    if response.usage:
        total_tokens = response.usage.total_tokens
        cache_hit = getattr(response.usage, "prompt_cache_hit_tokens", None)
        cache_miss = getattr(response.usage, "prompt_cache_miss_tokens", None)
        if cache_hit is not None or cache_miss is not None:
            logging.info(f"    - [KV Cache] å‘½ä¸­: {cache_hit or 0} tokens, æœªå‘½ä¸­: {cache_miss or 0} tokens")
    logging.info("    - Token usage (total): %s", total_tokens if total_tokens is not None else "unknown")

    logging.info(f"    - Raw content from model: {content[:80] if content else 'None'}...")

    if is_reasoner_model and reasoning_content:
        logging.info(f"    - [æ·±åº¦æ±‚ç´¢æ¨ç†å™¨] æå–åˆ°æ€è€ƒè¿‡ç¨‹ ({len(reasoning_content)} å­—ç¬¦): {reasoning_content[:500]}...")

    final_content = _clean_text_artifacts(content or "")

    logging.info(f"    - API è°ƒç”¨æˆåŠŸ ({duration:.2f}ç§’), æ¨¡å‹: {model_name}, æœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_content)} å­—ç¬¦.")
    try:
        tracker = get_tracker(config.task_id)
        if tracker:
            tracker.pulse(f"è°ƒç”¨æ¨¡å‹ {model_name} å®Œæˆï¼Œç”¨æ—¶ {duration:.1f}s")
    except Exception as e:
        logger.debug(f"è¿›åº¦è¿½è¸ªå™¨æ›´æ–°å¤±è´¥: {str(e)}")

    if not final_content or final_content.isspace():
        logging.warning(f"    - AI è°ƒç”¨è¿”å›ç©ºå†…å®¹ (æ¨¡å‹: {model_name})")
        if is_reasoner_model:
            raise EmptyResponseFromReasonerError(f"æ¨¡å‹ {model_name} è¿”å›ç©ºå†…å®¹ã€‚")

    return final_content


def _single_completion_with_meta(
    config: Config,
    model_name: str,
    messages: list[dict[str, str]],
    *,
    temperature: float,
    max_tokens: int,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
) -> tuple[str, str | None]:
    """
    æ‰§è¡Œä¸€æ¬¡éæµå¼è¡¥å…¨ï¼Œå¹¶è¿”å› (content, finish_reason)ã€‚ä»…ç”¨äºå†™ä½œå‹è¾“å‡ºçš„è‡ªåŠ¨ç»­å†™ç­–ç•¥ã€‚
    """
    client = _ensure_sync_client(config)
    call_params, _ = _build_chat_call_params(
        model_name,
        messages,
        max_tokens,
        temperature,
        top_p,
        frequency_penalty,
        presence_penalty,
    )
    resp = client.chat.completions.create(**call_params)
    choice = resp.choices[0]
    content = choice.message.content or ""
    finish_reason = getattr(choice, "finish_reason", None)
    return _clean_text_artifacts(content), finish_reason


def call_ai_writing_with_auto_continue(
    config: Config,
    model_name: str,
    messages: list[dict[str, str]],
    *,
    temperature: float | None = None,
    max_tokens_output: int = -1,
    top_p: float | None = None,
    frequency_penalty: float | None = None,
    presence_penalty: float | None = None,
    continuation_prompt: str = "è¯·ä»ä¸Šæ–‡ç»§ç»­ï¼Œè¡¥å®Œæœªå®Œæˆçš„å¥å­ä¸æ®µè½ï¼Œå®Œæˆè¯¥ç« èŠ‚ã€‚",
    max_continues: int = 1,
) -> str:
    """
    å†™ä½œå‹è°ƒç”¨ï¼šè‹¥ finish_reason == 'length'ï¼Œè‡ªåŠ¨ç»­å†™ä¸€æ¬¡ï¼ˆæœ€å¤š max_continues æ¬¡ï¼‰ã€‚
    ä»…ç”¨äºéç»“æ„åŒ–å†™ä½œè¾“å‡ºï¼ˆç« èŠ‚ç”Ÿæˆã€é•¿æ®µæ–‡æœ¬ç­‰ï¼‰ã€‚

    æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼šè‡ªåŠ¨è§„èŒƒåŒ–æ‰€æœ‰ messagesï¼Œç¡®ä¿ç¬¦åˆ API è¦æ±‚ã€‚
    """
    # è§„èŒƒåŒ–æ‰€æœ‰æ¶ˆæ¯ï¼Œé˜²æ­¢åŒ…å« tool_calls ç­‰å¤æ‚å­—æ®µ
    messages = _coerce_message_content(messages)

    final_temperature = temperature if temperature is not None else config.generation.temperature_creative
    final_top_p = top_p if top_p is not None else config.generation.top_p_creative
    final_frequency_penalty = frequency_penalty if frequency_penalty is not None else _default_frequency_penalty(config)
    final_presence_penalty = presence_penalty if presence_penalty is not None else _default_presence_penalty(config)

    # ä½¿ç”¨é›†ä¸­ç®¡ç†çš„æ¨¡å‹é™åˆ¶
    if max_tokens_output > 0:
        effective_max_output_tokens = min(max_tokens_output, ModelLimits.get_max_output(model_name))
    else:
        effective_max_output_tokens = ModelLimits.get_max_output(model_name)

    retry_exception_types = build_retry_exception_types()
    retryer = build_retryer(config, retry_exception_types)

    # ç¬¬ä¸€æ¬¡è°ƒç”¨
    content, finish_reason = retryer(
        _single_completion_with_meta,
        config,
        model_name,
        messages,
        temperature=final_temperature,
        max_tokens=effective_max_output_tokens,
        top_p=final_top_p,
        frequency_penalty=final_frequency_penalty,
        presence_penalty=final_presence_penalty,
    )

    if finish_reason == "length" and max_continues > 0:
        logging.info("æ£€æµ‹åˆ° finish_reason=lengthï¼Œè‡ªåŠ¨è§¦å‘ä¸€æ¬¡ç»­å†™ã€‚")
        cont_messages = [
            *messages,
            {"role": "assistant", "content": content},
            {"role": "user", "content": continuation_prompt},
        ]
        extra, _ = retryer(
            _single_completion_with_meta,
            config,
            model_name,
            cont_messages,
            temperature=final_temperature,
            max_tokens=effective_max_output_tokens,
            top_p=final_top_p,
            frequency_penalty=final_frequency_penalty,
            presence_penalty=final_presence_penalty,
        )
        content = (content or "") + ("\n" + extra if extra else "")

    return content or ""


def call_ai(
    config: Config,
    model_name: str,
    messages: list[dict[str, str]],
    temperature: float | None = None,
    max_tokens_output: int = -1,
    top_p: float | None = None,
    frequency_penalty: float | None = None,
    presence_penalty: float | None = None,
    response_format: dict[str, Any] | None = None,
    schema: type[Any] | None = None,
) -> Any | str:
    """
    å¸¦å¥å£®é‡è¯•æœºåˆ¶å’Œæ™ºèƒ½ token ç®¡ç†çš„ AI è°ƒç”¨å°è£…å‡½æ•° (å®Œå…¨åŒæ­¥ç‰ˆæœ¬)ã€‚

    æ–°å¢å‚æ•°:
        schema: å¯é€‰çš„ Pydantic BaseModel ç±»å‹ï¼Œç”¨äºç»“æ„åŒ–è¾“å‡º

    æ³¨æ„ï¼šæœ¬å‡½æ•°ä¼šè‡ªåŠ¨è§„èŒƒåŒ–æ‰€æœ‰ messagesï¼Œç§»é™¤ tool_calls/function_call ç­‰
    å¤æ‚å­—æ®µï¼Œç¡®ä¿ç¬¦åˆ OpenAI API è¦æ±‚ã€‚è°ƒç”¨è€…æ— éœ€æ‰‹åŠ¨è§„èŒƒåŒ–ã€‚
    """
    # è§„èŒƒåŒ–æ‰€æœ‰æ¶ˆæ¯ï¼Œç¡®ä¿æ²¡æœ‰ tool_calls/function_call ç­‰å¤æ‚å­—æ®µ
    # è¿™é˜²æ­¢äº†ä» API å“åº”ä¸­è·å–çš„æ¶ˆæ¯ï¼ˆå¯èƒ½åŒ…å« tool_callsï¼‰è¢«ç›´æ¥ä¼ é€’ç»™ä¸‹æ¸¸
    messages = _coerce_message_content(messages)

    final_temperature = temperature if temperature is not None else config.generation.temperature_factual
    final_top_p = top_p if top_p is not None else config.generation.top_p_factual
    final_frequency_penalty = frequency_penalty if frequency_penalty is not None else _default_frequency_penalty(config)
    final_presence_penalty = presence_penalty if presence_penalty is not None else _default_presence_penalty(config)

    # ä½¿ç”¨é›†ä¸­ç®¡ç†çš„æ¨¡å‹é™åˆ¶
    if max_tokens_output > 0:
        effective_max_output_tokens = min(max_tokens_output, ModelLimits.get_max_output(model_name))
    else:
        effective_max_output_tokens = ModelLimits.get_max_output(model_name)

    is_reasoner_model = "reasoner" in model_name.lower()
    if is_reasoner_model and effective_max_output_tokens < ModelLimits.REASONER_MIN_TOKENS:
        logging.info(f"    - ä¸º {model_name} è°ƒæ•´ max_tokens_output è‡³ {ModelLimits.REASONER_MIN_TOKENS} (ä»¥å®¹çº³æ€ç»´é“¾)ã€‚")
        effective_max_output_tokens = ModelLimits.REASONER_MIN_TOKENS

    # è‹¥éœ€è¦ JSON å¼ºåˆ¶æ ¼å¼ï¼Œè‡ªåŠ¨æ³¨å…¥ JSON æç¤ºï¼Œé¿å… 400 é”™è¯¯
    effective_messages = messages
    if response_format and isinstance(response_format, dict) and response_format.get("type") == "json_object":
        effective_messages = _ensure_json_instruction(messages)

    total_input_tokens = sum(config.count_tokens(m.get("content", "")) for m in effective_messages)
    logging.info(f"    - AI è°ƒç”¨: æ¨¡å‹={model_name}, è¾“å…¥ Tokens (ä¼°ç®—): {total_input_tokens}, è¯·æ±‚è¾“å‡º Tokens: {max_tokens_output} -> æœ‰æ•ˆæœ€å¤§å€¼: {effective_max_output_tokens}")

    model_context_limit = ModelLimits.get_context_limit(model_name)
    if total_input_tokens + effective_max_output_tokens > model_context_limit:
        logging.warning(f"    - è­¦å‘Š: è¾“å…¥+è¾“å‡º Tokens ({total_input_tokens + effective_max_output_tokens}) å¯èƒ½è¶…è¿‡æ¨¡å‹ {model_name} çš„ä¸Šä¸‹æ–‡é™åˆ¶ ({model_context_limit})ã€‚")
        available_for_output = model_context_limit - total_input_tokens
        if available_for_output < effective_max_output_tokens:
            new_max_output = max(100, available_for_output - 100)
            logging.info(f"    - è°ƒæ•´ max_tokens_output ä» {effective_max_output_tokens} åˆ° {new_max_output} ä»¥é€‚åº”ä¸Šä¸‹æ–‡ã€‚")
            effective_max_output_tokens = new_max_output
        if effective_max_output_tokens <= 0:
            logging.error(f"    - ä¸¥é‡é”™è¯¯: æ¨¡å‹ {model_name} æ²¡æœ‰å¯ç”¨çš„è¾“å‡ºä»¤ç‰Œã€‚è¾“å…¥ä»¤ç‰Œ: {total_input_tokens}, ä¸Šä¸‹æ–‡é™åˆ¶: {model_context_limit}")
            return "AIæ¨¡å‹è°ƒç”¨å¤±è´¥ (é”™è¯¯): è¾“å…¥å†…å®¹å·²å æ»¡ä¸Šä¸‹æ–‡çª—å£ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚"

    retry_exception_types = build_retry_exception_types()

    # å¦‚æœæœ‰ schemaï¼Œå…ˆå°è¯•ç»“æ„åŒ–è°ƒç”¨
    if schema is not None:
        try:
            schema_kwargs: dict[str, Any] = {
                "temperature": final_temperature,
                "max_tokens": effective_max_output_tokens,
                "top_p": final_top_p,
                "frequency_penalty": final_frequency_penalty,
                "presence_penalty": final_presence_penalty,
            }
            result, status = call_ai_with_schema(config, model_name, messages, schema, schema_kwargs)
            if status in ["success", "fallback_success"]:
                return result
            elif status == "instructor_unavailable":
                logging.warning("Instructorä¸å¯ç”¨ï¼Œç»§ç»­æ™®é€šè°ƒç”¨")
            elif status == "fallback_failed":
                logging.warning("ç»“æ„åŒ–è°ƒç”¨å®Œå…¨å¤±è´¥ï¼Œç»§ç»­æ™®é€šé‡è¯•æœºåˆ¶")
        except Exception as e:
            logging.warning(f"ç»“æ„åŒ–è°ƒç”¨å¼‚å¸¸: {e}ï¼Œç»§ç»­æ™®é€šé‡è¯•æœºåˆ¶")

    retryer = build_retryer(config, retry_exception_types)

    try:
        # Pulse before entering retry loop
        try:
            tracker = get_tracker(config.task_id)
            if tracker:
                tracker.pulse(f"æ­£åœ¨è¯·æ±‚ {model_name}ï¼ˆé¢„è®¡è¾“å‡ºä¸Šé™ {effective_max_output_tokens} tokensï¼‰")
        except Exception as e:
            logger.debug(f"è¿›åº¦è¿½è¸ªå™¨è„‰å†²å¤±è´¥: {str(e)}")
        return retryer(
            call_ai_core,
            config,
            model_name,
            effective_messages,
            final_temperature,
            effective_max_output_tokens,
            final_top_p,
            final_frequency_penalty,
            final_presence_penalty,
            response_format=response_format,
        )
    except openai.APIStatusError as e:
        logging.error(f"    - æ¨¡å‹ {model_name} çš„ API è°ƒç”¨çŠ¶æ€é”™è¯¯ (æœªé‡è¯•æˆ–æœ€ç»ˆå°è¯•å¤±è´¥): {e.status_code} - {e.response.text if e.response else 'æ— å“åº”æ–‡æœ¬'}")
        # å½“æœåŠ¡ç«¯ä¸æ”¯æŒ JSON å¼ºåˆ¶æ ¼å¼æ—¶ï¼Œç§»é™¤ response_format é‡è¯•ä¸€æ¬¡
        if e.status_code == 400 and response_format is not None:
            logging.warning("    - æ”¶åˆ° 400 ä¸”ä½¿ç”¨äº† response_formatï¼Œå°è¯•ç§»é™¤ response_format åé‡è¯•ä¸€æ¬¡ã€‚")
            try:
                return retryer(
                    call_ai_core,
                    config,
                    model_name,
                    messages,
                    final_temperature,
                    effective_max_output_tokens,
                    final_top_p,
                    final_frequency_penalty,
                    final_presence_penalty,
                    response_format=None,
                )
            except Exception as e_retry:
                logging.error(
                    f"    - æ—  response_format é‡è¯•ä»å¤±è´¥: {e_retry}",
                    exc_info=True,
                )
        if e.status_code == 400:
            logging.error(f"æç¤ºï¼šè¯·æ±‚å¯èƒ½æ— æ•ˆ (ä¾‹å¦‚ï¼Œè¾“å…¥ä»¤ç‰Œ {total_input_tokens} + è¾“å‡º {effective_max_output_tokens} è¶…å‡ºæ¨¡å‹é™åˆ¶)ã€‚è¿™æ˜¯ä¸€ä¸ªä¸å¯é‡è¯•çš„å®¢æˆ·ç«¯é”™è¯¯ã€‚")
        error_message_detail = "æœªçŸ¥é”™è¯¯"
        if e.response is not None:
            try:
                error_message_detail = e.response.json().get("error", {}).get("message", "æœªçŸ¥é”™è¯¯")
            except json.JSONDecodeError:
                error_message_detail = e.response.text if e.response.text else "æ— å“åº”æ–‡æœ¬"
        return f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥ (API é”™è¯¯ {e.status_code}): {error_message_detail}"
    except Exception as e:
        logging.error(f"    - æ¨¡å‹ {model_name} çš„ AI è°ƒç”¨å› æœªå¤„ç†çš„å¼‚å¸¸æˆ–æ‰€æœ‰é‡è¯•åå¤±è´¥: {e}", exc_info=True)
        return "AIæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€APIå¯†é’¥æˆ–ç›¸å…³è®¾ç½®ï¼Œæˆ–æŸ¥çœ‹è¯¦ç»†æ—¥å¿—ã€‚"


def preflight_llm_connectivity(config: Config, *, model_name: str | None = None) -> bool:
    """æ‰§è¡Œä¸€æ¬¡è½»é‡çš„è¿é€šæ€§é¢„æ£€ï¼Œå¿«é€Ÿåé¦ˆä»£ç†/TLSå¼‚å¸¸ã€‚

    è¿”å› True è¡¨ç¤ºå¯ç”¨ï¼›False è¡¨ç¤ºå¤±è´¥ï¼ˆè°ƒç”¨æ–¹å¯ç»ˆæ­¢å¹¶æç¤ºç”¨æˆ·ä¿®å¤ç¯å¢ƒï¼‰ã€‚
    """
    try:
        test_model = model_name or config.models.main_ai_model
        messages = [{"role": "user", "content": "ping"}]
        # ä½æ¸©åº¦ã€æçŸ­è¾“å‡ºï¼Œå°½é‡å‡å°‘ç­‰å¾…æ—¶é—´
        resp = call_ai(
            config,
            test_model,
            messages,
            temperature=0.0,
            max_tokens_output=8,
        )
        if resp and "AIæ¨¡å‹è°ƒç”¨å¤±è´¥" not in resp:
            logging.info("LLMé¢„æ£€é€šè¿‡: æ¨¡å‹=%s", test_model)
            return True
        logging.error("LLMé¢„æ£€å¤±è´¥: %s", resp)
        return False
    except Exception as exc:
        logging.error("LLMé¢„æ£€å‘ç”Ÿå¼‚å¸¸: %s", exc, exc_info=True)
        return False
