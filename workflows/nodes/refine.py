from __future__ import annotations

import json
import logging
import re
from collections.abc import Iterable
from difflib import SequenceMatcher
from typing import Any

from pydantic import ValidationError

from core.context_manager import ContextManager
from core.progress import StepOutput, step_result, workflow_step
from core.state_manager import WorkflowStateAdapter
from planning.tool_definitions import CritiqueModel, FineGrainedPatchList
from services.llm_interaction import call_ai_with_schema
from utils.progress_tracker import safe_pulse
from utils.text_processor import extract_json_from_ai_response
from workflows.graph_state import GraphState
from workflows.prompts import PATCH_SCHEMA_INSTRUCTIONS


@workflow_step("refine_node", "ç”Ÿæˆå†…å®¹ä¼˜åŒ–è¡¥ä¸")
def refine_node(state: GraphState) -> StepOutput:
    workflow_state = WorkflowStateAdapter.ensure(state)
    config = workflow_state.config
    current_iteration = workflow_state.refinement_count + 1
    max_rounds = config.max_refinement_rounds
    logging.info(
        "[RefineLoop] Iteration %s/%s -> refine_node",
        current_iteration,
        max_rounds,
    )
    safe_pulse(
        config.task_id,
        f"è¿­ä»£ {current_iteration}/{max_rounds} Â· ç”Ÿæˆå†…å®¹ä¼˜åŒ–è¡¥ä¸ä¸­...",
    )

    draft_content = workflow_state.draft_content or ""
    critique = workflow_state.critique or ""
    research_brief = workflow_state.research_brief or ""
    style_guide = workflow_state.style_guide or ""
    outline_data = workflow_state.outline

    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–ç ”ç©¶æ•°æ®
    structured_research_data = getattr(workflow_state, "structured_research_data", None)

    if not isinstance(outline_data, dict) or not outline_data.get("outline"):
        logging.warning("refine_node: Outline æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡è¡¥ä¸ç”Ÿæˆã€‚")
        return step_result({"patches": []}, "ç¼ºå°‘å¤§çº²")

    external_data = workflow_state.external_data or ""
    raw_structured_critique = getattr(workflow_state, "structured_critique", None)
    structured_critique: CritiqueModel | None = None
    if raw_structured_critique:
        try:
            structured_critique = CritiqueModel.model_validate(raw_structured_critique)
        except (ValidationError, TypeError) as exc:
            logging.warning("refine_node: æ— æ³•è§£æç»“æ„åŒ–è¯„å®¡æ•°æ®ï¼Œæ”¹ç”¨æ–‡æœ¬å›é€€ã€‚è¯¦æƒ…: %s", exc)
    knowledge_gaps: list[str] = []
    if structured_critique and structured_critique.knowledge_gaps:
        knowledge_gaps = structured_critique.knowledge_gaps
    else:
        knowledge_gaps = workflow_state.knowledge_gaps or []

    if not draft_content:
        logging.warning("refine_node: No draft_content found to refine. Skipping.")
        return step_result({"patches": []}, "ç¼ºå°‘è‰ç¨¿å†…å®¹")

    embedding_model = getattr(config, "embedding_model_instance", None)

    # å¤„ç†ç ”ç©¶æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–æ•°æ®ï¼Œå¦åˆ™ä½¿ç”¨ä¼ ç»Ÿå­—ç¬¦ä¸²æ ¼å¼
    research_summary = ""
    research_context = ""
    if structured_research_data:
        research_summary, research_context = _process_structured_research_data(structured_research_data)
    else:
        research_summary = research_brief or ""
        research_context = research_brief or ""

    combined_context_segments = [segment.strip() for segment in (external_data, research_context) if segment and segment.strip()]
    combined_context_data = "\n\n".join(combined_context_segments)
    context_manager_for_patch = ContextManager(
        config,
        style_guide,
        outline_data,
        combined_context_data,
        embedding_model,
        repository=workflow_state.context_repository,
        rag_service=workflow_state.rag_service,
        assembler=workflow_state.context_assembler,
    )

    outline_chapters = outline_data.get("outline", [])

    # ğŸ”§ ä¿®å¤ï¼šä» draft_content ä¸­åŠ¨æ€æå–å®é™…å­˜åœ¨çš„ section_idï¼ˆè€Œéä» outline_dataï¼‰
    # è¿™æ ·ä¿è¯æ˜ å°„è¡¨å§‹ç»ˆä¸å½“å‰æ–‡æ¡£å†…å®¹åŒæ­¥
    actual_section_ids_in_draft = re.findall(r"<!--\s*section_id:\s*([A-Za-z0-9-]+)\s*-->", draft_content)

    if not actual_section_ids_in_draft:
        logging.warning("  âš ï¸  draft_content ä¸­æœªæ‰¾åˆ°ä»»ä½• section_id æ³¨é‡Šï¼Œå›é€€åˆ° outline_data")
        # å›é€€æ–¹æ¡ˆï¼šä» outline_data ç”Ÿæˆæ˜ å°„è¡¨
        all_chapters_index = _build_chapter_index(outline_chapters)
        section_number_map = {idx + 1: item["id"] for idx, item in enumerate(all_chapters_index) if item.get("id")}
    else:
        # æ­£å¸¸æµç¨‹ï¼šæ ¹æ® draft ä¸­å®é™…å‡ºç°çš„é¡ºåºç”Ÿæˆæ˜ å°„è¡¨
        section_number_map = {idx + 1: section_id for idx, section_id in enumerate(actual_section_ids_in_draft)}
        logging.info("  âœ… ä» draft_content ä¸­æå– %d ä¸ªç« èŠ‚IDï¼Œç”Ÿæˆæ•°å­—æ˜ å°„è¡¨", len(section_number_map))

    target_chapters = _select_target_chapters(outline_chapters, structured_critique, knowledge_gaps, critique)
    logging.info(
        "  - Patcher ç›®æ ‡ç« èŠ‚: %s",
        ", ".join(f"{item['title']}" for item in target_chapters) if target_chapters else "æ— ",
    )

    # ä¸º AI Prompt ç”Ÿæˆ target_chapters åœ¨å…¨å±€ç¼–å·ä¸­çš„ä½ç½®
    target_chapter_ids = {item["id"] for item in target_chapters if item.get("id")}
    target_global_numbers = {num: uuid for num, uuid in section_number_map.items() if uuid in target_chapter_ids}

    chapter_contexts: list[str] = []
    # ğŸ”§ ä½¿ç”¨å…¨å±€ç¼–å·è€Œéå±€éƒ¨ idx
    for chapter_info in target_chapters:
        chapter_title = chapter_info["title"]
        chapter_id = chapter_info.get("id") or ""
        chapter_path = chapter_info.get("path") or chapter_title
        existing_text = _chapter_body_snippet(draft_content, chapter_id)

        # ğŸ”§ ä»å†…å®¹ä¸­ç§»é™¤ UUID æ³¨é‡Šï¼Œé¿å… AI æ··æ·†
        existing_text_clean = re.sub(r"<!--\s*section_id:\s*[A-Za-z0-9-]+\s*-->", "", existing_text)

        # æŸ¥æ‰¾è¯¥ç« èŠ‚çš„å…¨å±€ç¼–å·
        global_num = next((num for num, uuid in section_number_map.items() if uuid == chapter_id), "?")

        context_packet = context_manager_for_patch.get_context_for_chapter_critique(chapter_title, draft_content, section_number_map)
        block_parts = [
            f"[Section #{global_num}] {chapter_path}",  # âœ… ä½¿ç”¨å…¨å±€ç¨³å®šç¼–å·
            "[Existing Draft]\n" + (existing_text_clean or "<æœªæ‰¾åˆ°è¯¥ç« èŠ‚æ­£æ–‡æˆ–ç« èŠ‚ä¸ºç©º>"),
        ]
        if context_packet:
            block_parts.append("[Supporting Context]\n" + context_packet)
        chapter_contexts.append("\n\n".join(block_parts))
    safe_pulse(
        config.task_id,
        f"è¿­ä»£ {current_iteration}/{max_rounds} Â· Patcherä¸Šä¸‹æ–‡å·²å‡†å¤‡ï¼Œç›®æ ‡ç« èŠ‚ {len(target_chapters)} ä¸ªï¼›è°ƒç”¨æ¨¡å‹ä¸­...",
    )

    precise_context_for_patcher = "\n\n".join(chapter_contexts)
    issues_for_prompt: list[str] = []
    if structured_critique:
        for issue in structured_critique.priority_issues or []:
            _append_unique_text(issues_for_prompt, issue)
        for gap in structured_critique.knowledge_gaps or []:
            _append_unique_text(issues_for_prompt, gap)
    for gap in knowledge_gaps:
        _append_unique_text(issues_for_prompt, gap)
    knowledge_gap_block = "\n".join(f"- {item}" for item in issues_for_prompt) if issues_for_prompt else "None"

    # ä¸º AI ç”Ÿæˆç®€å•æ˜“ç”¨çš„ç« èŠ‚åˆ—è¡¨ï¼ˆä½¿ç”¨å…¨å±€ç¨³å®šç¼–å·ï¼‰
    section_list_for_ai = []
    for num, uuid in sorted(target_global_numbers.items()):
        # ä» target_chapters ä¸­æ‰¾åˆ°å¯¹åº”ç« èŠ‚
        chapter_info = next((ch for ch in target_chapters if ch.get("id") == uuid), None)
        if chapter_info:
            chapter_title = chapter_info.get("title", "æœªå‘½å")
            section_list_for_ai.append(f"  [{num}] {chapter_title[:80]}")

    section_reference_block = "\n".join(section_list_for_ai) if section_list_for_ai else "  (æ— å¯ç”¨ç« èŠ‚)"

    # ğŸ”§ Phase 1 ä¼˜åŒ–ï¼šè®¡ç®—é—®é¢˜æ•°é‡å¹¶å¼ºè°ƒè¦æ±‚
    total_issues_count = len(issues_for_prompt)

    patch_user_prompt = (
        f"""[Original Problem]\n{config.user_problem}\n\n"""
        f"""[Latest Research Brief]\n{(research_summary or research_brief or "None")}\n\n"""
        f"""[Revision Feedback]\n---\n{critique}\n---\n\n"""
        f"""[Knowledge Gaps]\n{knowledge_gap_block}\n\n"""
        f"""[Available Sections for Editing]\n"""
        f"""Use the section number as target_id:\n"""
        f"""{section_reference_block}\n\n"""
        f"""Example: To edit section [1], use "target_id": 1\n"""
        f"""Example: To edit section [3], use "target_id": 3\n\n"""
        f"""[Target Chapter Context for Revision]\n---\n{precise_context_for_patcher}\n---\n\n"""
        f"""[TASK REQUIREMENTS]\n"""
        f"""Total issues identified: {total_issues_count}\n"""
        f"""You MUST generate AT LEAST {total_issues_count} patches (one per issue).\n"""
        f"""Do NOT skip any issues. Address each one with a specific sentence-level edit.\n\n"""
        """Now generate the patch list to resolve these issues. Use simple numeric target_id."""
    )

    # ğŸ” è°ƒè¯•ï¼šè®°å½•å®Œæ•´ prompt åˆ°æ–‡ä»¶
    if getattr(config, "debug_prompts", False):
        import datetime

        debug_file = f"debug_patch_prompt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write("SYSTEM PROMPT\n")
            f.write("=" * 80 + "\n")
            f.write("You are Patch-Bot. Generate sentence-level revision patches.\n")
            f.write(PATCH_SCHEMA_INSTRUCTIONS)
            f.write("\nEach patch must address issues from the critique and knowledge gaps only.\n\n")
            f.write("=" * 80 + "\n")
            f.write("USER PROMPT\n")
            f.write("=" * 80 + "\n")
            f.write(patch_user_prompt)
            f.write("\n" + "=" * 80 + "\n")
            f.write(f"Section Number Map: {section_number_map}\n")
        logging.info(f"  - Prompt è°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: {debug_file}")

    model_name = config.patcher_model_name or getattr(config, "main_ai_model", None)
    if not model_name:
        raise ValueError("æœªé…ç½® patcher æ¨¡å‹åç§°ï¼Œæ— æ³•ç”Ÿæˆè¡¥ä¸ã€‚")

    def _invoke_patch_request(prompt: str, temperature: float, attempt_label: str) -> tuple[list[dict[str, Any]], str | None]:
        try:
            # ğŸ”§ ä½¿ç”¨ç»“æ„åŒ–è°ƒç”¨ï¼ŒAI ä¼šçœ‹åˆ° Pydantic Schemaï¼ˆtarget_id å¿…é¡»æ˜¯ intï¼‰
            # ğŸ†• ä¼˜åŒ–ï¼šå¼ºè°ƒå¿…é¡»ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆè¡¥ä¸
            system_prompt = (
                "You are Patch-Bot. Generate sentence-level revision patches.\n\n"
                + PATCH_SCHEMA_INSTRUCTIONS
                + "\n\n**CRITICAL REQUIREMENTS:**\n"
                + "1. target_id MUST be an integer (1, 2, 3, 4, 5, ...). NEVER use UUID strings.\n"
                + "2. Generate AT LEAST ONE patch for EACH issue in the critique and knowledge gaps.\n"
                + "3. DO NOT skip issues or consolidate multiple issues into one patch.\n"
                + "4. If there are N issues, generate AT LEAST N patches.\n"
                + "5. Each patch should address ONE specific issue clearly.\n\n"
                + "If you cannot fix an issue with sentence edits, still create a patch that attempts improvement."
            )

            edit_obj, call_mode = call_ai_with_schema(
                config,
                model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ],
                schema=FineGrainedPatchList,
                kwargs={
                    "temperature": temperature,
                    "max_tokens_output": getattr(config, "intermediate_edit_max_tokens", 2048),
                },
            )

            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼ˆå›é€€åˆ°æ™®é€šè°ƒç”¨ï¼‰ï¼Œå°è¯•æ‰‹åŠ¨è§£æ
            if isinstance(edit_obj, str):
                json_response_text = extract_json_from_ai_response(
                    config,
                    edit_obj,
                    context_for_error_log=f"Patcher AI response ({attempt_label})",
                )
                if not json_response_text:
                    logging.info("  - [%s] æ¨¡å‹æœªè¿”å›è¡¥ä¸ç»“æœã€‚", attempt_label)
                    return [], None

                parsed_payload = json.loads(json_response_text)
                if isinstance(parsed_payload, list):
                    logging.warning(
                        "  - [%s] æ”¶åˆ°åˆ—è¡¨å½¢å¼è¡¥ä¸ï¼Œè‡ªåŠ¨å°è£…ä¸º {'patches': ...}ã€‚",
                        attempt_label,
                    )
                    parsed_payload = {"patches": parsed_payload}
                    json_response_text = json.dumps(parsed_payload, ensure_ascii=False)

                edit_obj = FineGrainedPatchList.model_validate_json(json_response_text)

            patch_list = [patch.model_dump() for patch in edit_obj.patches]
            logging.info("  - [%s] æˆåŠŸç”Ÿæˆ %s ä¸ªè¡¥ä¸ (æ¨¡å¼: %s)", attempt_label, len(patch_list), call_mode)
            # ğŸ” è°ƒè¯•ï¼šè®°å½•AIè¿”å›çš„ target_id
            if patch_list:
                target_ids = [p.get("target_id") for p in patch_list]
                logging.debug("  - [%s] AIè¿”å›çš„ target_id åˆ—è¡¨: %s (ç±»å‹: %s)", attempt_label, target_ids, [type(tid).__name__ for tid in target_ids])
            return patch_list, None
        except (ValidationError, json.JSONDecodeError) as exc:
            logging.error("  - [%s] è¡¥ä¸ç”Ÿæˆå¤±è´¥: %s", attempt_label, exc)
            return [], str(exc)

    # ğŸ†• ä¼˜åŒ–ï¼šç¡®ä¿æœ€ä½æ¸©åº¦ï¼Œæé«˜ç”Ÿæˆå¤šæ ·æ€§
    # ğŸ”§ Phase 1 ä¼˜åŒ–ï¼šæé«˜åˆå§‹æ¸©åº¦ä»¥å¢åŠ è¡¥ä¸å¤šæ ·æ€§
    base_temperature = max(config.temperature_factual, 0.4)  # ä» 0.3 æå‡åˆ° 0.4
    patches, _ = _invoke_patch_request(patch_user_prompt, base_temperature, "primary")
    retry_attempted = False

    # ğŸ†• ä¼˜åŒ–ï¼šè®¡ç®—æœŸæœ›çš„æœ€å°è¡¥ä¸æ•°
    expected_min_patches = max(len(knowledge_gaps), len(issues_for_prompt), 1)

    # ğŸ†• ä¼˜åŒ–ï¼šä¸ä»…æ£€æŸ¥æ˜¯å¦ä¸ºç©ºï¼Œè¿˜æ£€æŸ¥æ•°é‡æ˜¯å¦å……è¶³
    patch_count_insufficient = len(patches) < expected_min_patches
    has_issues_to_fix = bool(issues_for_prompt or knowledge_gaps or critique.strip())
    should_retry = patch_count_insufficient and getattr(config, "enable_patch_retry", False) and has_issues_to_fix
    if should_retry:
        retry_attempted = True
        # ğŸ”§ Phase 1 ä¼˜åŒ–ï¼šè¿›ä¸€æ­¥æé«˜é‡è¯•æ¸©åº¦
        retry_temperature = min(base_temperature + 0.3, 0.95)  # ä» +0.2 æå‡åˆ° +0.3ï¼Œä¸Šé™ä» 0.9 åˆ° 0.95

        # ğŸ†• ä¼˜åŒ–ï¼šæ ¹æ®æƒ…å†µå®šåˆ¶é‡è¯•æç¤º
        if not patches:
            retry_reason = "æœªç”Ÿæˆä»»ä½•è¡¥ä¸"
            retry_directive = (
                "\n\n[RETRY DIRECTIVE - CRITICAL]\n"
                f"The previous attempt returned NO patches, but there are {expected_min_patches} issues to address.\n\n"
                "**MANDATORY REQUIREMENTS:**\n"
                f"- You MUST generate at least {expected_min_patches} patches\n"
                "- Create ONE patch for EACH issue listed in critique and knowledge gaps\n"
                "- Do NOT consolidate multiple issues into one patch\n"
                "- If unsure how to fix, still propose an improvement attempt\n\n"
                "Re-evaluate ALL critique points and knowledge gaps one by one.\n"
                "Generate a separate patch for each item."
            )
        else:
            retry_reason = f"è¡¥ä¸æ•°é‡ä¸è¶³ï¼ˆç”Ÿæˆäº† {len(patches)} ä¸ªï¼Œé¢„æœŸè‡³å°‘ {expected_min_patches} ä¸ªï¼‰"
            missing_count = expected_min_patches - len(patches)
            retry_directive = (
                "\n\n[RETRY DIRECTIVE - INSUFFICIENT PATCHES]\n"
                f"Previous attempt: {len(patches)} patches generated\n"
                f"Expected minimum: {expected_min_patches} patches\n"
                f"Missing: {missing_count} patches\n\n"
                "**ACTION REQUIRED:**\n"
                f"- Generate {missing_count} MORE patches to address remaining issues\n"
                "- Review the critique and knowledge gaps list\n"
                "- Identify which issues were NOT addressed in the first attempt\n"
                "- Create patches for ALL unaddressed issues\n\n"
                "Return ALL patches (previous + new ones) in your response."
            )

        retry_prompt = patch_user_prompt + retry_directive

        safe_pulse(
            config.task_id,
            f"è¿­ä»£ {current_iteration}/{max_rounds} Â· {retry_reason}ï¼Œé‡è¯•ç”Ÿæˆä¸­...",
        )
        logging.warning("=" * 60)
        logging.warning("âš ï¸  è¡¥ä¸ç”Ÿæˆä¸å……åˆ†ï¼Œå°è¯•é‡è¯•")
        logging.warning(f"  - å½“å‰è¿­ä»£: {current_iteration}/{max_rounds}")
        logging.warning(f"  - ç”Ÿæˆçš„è¡¥ä¸æ•°: {len(patches)}")
        logging.warning(f"  - é¢„æœŸæœ€å°æ•°é‡: {expected_min_patches}")
        logging.warning(f"  - Knowledge gaps: {len(knowledge_gaps)} ä¸ª")
        logging.warning(f"  - Critique issues: {len(issues_for_prompt)} ä¸ª")
        logging.warning(f"  - é‡è¯•æ¸©åº¦: {retry_temperature}")
        logging.warning("=" * 60)

        retry_patches, _ = _invoke_patch_request(retry_prompt, retry_temperature, "retry")
        if retry_patches:
            logging.info(f"  âœ“ é‡è¯•æˆåŠŸï¼Œæ–°å¢ {len(retry_patches)} ä¸ªè¡¥ä¸")
            patches = retry_patches
        else:
            logging.warning("  âœ— é‡è¯•å¤±è´¥ï¼Œä»æœªç”Ÿæˆè¡¥ä¸")

    # ğŸ” è°ƒè¯•ï¼šè®°å½•æœ€ç»ˆçš„è¡¥ä¸å’Œæ˜ å°„è¡¨
    if patches:
        logging.info("  - è¡¥ä¸ target_id åˆ—è¡¨: %s", [p.get("target_id") for p in patches])
    logging.info("  - section_number_map æ˜ å°„è¡¨: %s", {k: v[:8] + "..." for k, v in section_number_map.items()} if section_number_map else "ç©º")

    detail_msg = f"è¿­ä»£ {current_iteration}/{max_rounds}ï¼Œç”Ÿæˆè¡¥ä¸ {len(patches)} ä¸ª"
    if retry_attempted:
        detail_msg += "ï¼ˆé‡è¯•" + ("æˆåŠŸ" if patches else "æ— è¡¥ä¸") + "ï¼‰"

    repository, rag_service, assembler = context_manager_for_patch.export_components()
    return step_result(
        {
            "patches": patches,
            "section_number_map": section_number_map,  # ä¼ é€’æ•°å­—â†’UUIDæ˜ å°„è¡¨
            "context_repository": repository,
            "rag_service": rag_service,
            "context_assembler": assembler,
            "knowledge_gaps": knowledge_gaps,
            "structured_critique": raw_structured_critique,
        },
        detail_msg,
    )


def _process_structured_research_data(structured_research_data: dict) -> tuple[str, str]:
    """
    å¤„ç†ç»“æ„åŒ–ç ”ç©¶æ•°æ®ï¼Œç”Ÿæˆç”¨äºè¡¥ä¸ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å†…å®¹
    """
    if not structured_research_data:
        return "", ""

    briefs = structured_research_data.get("briefs", [])
    statistics = structured_research_data.get("statistics", {})

    if not briefs:
        return "", ""

    summary_lines: list[str] = []
    detail_lines: list[str] = []
    detail_lines.append("ç»“æ„åŒ–ç ”ç©¶æ•°æ®ï¼ˆåŸºäºçŸ¥è¯†ç¼ºå£æœç´¢ç»“æœï¼‰:")
    detail_lines.append("")

    sorted_briefs = sorted(
        briefs,
        key=lambda x: (x.get("source_quality", "") == "high", x.get("confidence", 0)),
        reverse=True,
    )

    for i, brief in enumerate(sorted_briefs[:10]):
        url = brief.get("url", "")
        title = brief.get("title", "")
        summary = brief.get("summary", "")
        key_points = brief.get("key_points", [])
        confidence = brief.get("confidence", 0)
        source_quality = brief.get("source_quality", "unknown")
        relevance = brief.get("relevance_score", 0)

        research_part = f"[ç ”ç©¶æº {i + 1}] {title or url or 'æœªå‘½åæ¥æº'}"
        if url:
            research_part += f" ({url})"
        research_part += f"\n  ç½®ä¿¡åº¦: {confidence:.2f} | è´¨é‡: {source_quality} | ç›¸å…³æ€§: {relevance:.2f}"
        research_part += f"\n  æ€»ç»“: {summary}"

        if key_points:
            research_part += "\n  å…³é”®è¦ç‚¹:"
            for point in key_points:
                research_part += f"\n    - {point}"

        detail_lines.append(research_part)
        detail_lines.append("")

        if title or summary:
            summary_excerpt = summary.strip() if summary else ""
            summary_lines.append(f"- {title or url}: {summary_excerpt[:160]}")

    if statistics:
        detail_lines.append("ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in statistics.items():
            detail_lines.append(f"- {key}: {value}")

    summary_text = "\n".join(summary_lines[:5])
    detail_text = "\n".join(detail_lines)
    return summary_text, detail_text


def _append_unique_text(collection: list[str], text: str | None) -> None:
    if not text:
        return
    candidate = text.strip()
    if not candidate:
        return
    if candidate not in collection:
        collection.append(candidate)


def _select_target_chapters(
    outline_chapters: Iterable[dict[str, Any]],
    structured_critique: CritiqueModel | None,
    knowledge_gaps: list[str],
    critique_text: str,
) -> list[dict[str, Any]]:
    chapters = _build_chapter_index(outline_chapters)
    if not chapters:
        return []

    score_map: dict[str, float] = {}

    def _bump_score(entry_id: str, weight: float) -> None:
        if not entry_id:
            return
        score_map[entry_id] = score_map.get(entry_id, 0.0) + weight

    def _match_text(text: str, base_weight: float = 1.0) -> None:
        if not text:
            return
        text_lower = text.lower()
        for entry in chapters:
            if entry["id_lower"] and entry["id_lower"] in text_lower:
                _bump_score(entry["id"], 3.0 * base_weight)
            if entry["title_lower"] and entry["title_lower"] in text_lower:
                _bump_score(entry["id"], 2.0 * base_weight)
            if entry["path_lower"] and entry["path_lower"] in text_lower:
                _bump_score(entry["id"], 1.2 * base_weight)
        best_entry = _best_fuzzy_match(text_lower, chapters)
        if best_entry:
            ratio = _similarity_ratio(text_lower, best_entry["title_lower"])
            _bump_score(best_entry["id"], max(1.0, ratio * 2.5) * base_weight)

    if structured_critique:
        for item in structured_critique.priority_issues or []:
            _match_text(item, base_weight=1.5)
        for item in structured_critique.knowledge_gaps or []:
            _match_text(item, base_weight=1.3)

    if not score_map:
        for gap in knowledge_gaps:
            _match_text(gap, base_weight=1.1)

    if not score_map and critique_text:
        _match_text(critique_text, base_weight=1.0)

    ranked = sorted(
        (entry for entry in chapters if score_map.get(entry["id"], 0) > 0),
        key=lambda item: (-score_map[item["id"]], item["order"]),
    )
    if ranked:
        max_targets = min(5, len(ranked))
        logging.debug(
            "refine_node: ç« èŠ‚è¯„åˆ†åˆ†å¸ƒ: %s",
            ", ".join(f"{entry['id']}={score_map[entry['id']]:.2f}" for entry in ranked[:max_targets]),
        )
        logging.info(
            "refine_node: è¯†åˆ«åˆ° %s ä¸ªå€™é€‰ç« èŠ‚ç”¨äºä¿®è®¢ï¼š%s",
            max_targets,
            ", ".join(r["path"] for r in ranked[:max_targets]),
        )
        return ranked[:max_targets]

    if chapters:
        fallback_targets = chapters[: min(5, len(chapters))]
        logging.info(
            "refine_node: æœªä»è¯„å®¡ä¸­è¯†åˆ«åˆ°ç‰¹å®šç« èŠ‚ï¼Œå›é€€åˆ°å‰ %s ä¸ªç« èŠ‚ä¸Šä¸‹æ–‡ã€‚",
            len(fallback_targets),
        )
        logging.debug(
            "refine_node: å›é€€ç« èŠ‚åˆ—è¡¨: %s",
            ", ".join(item["path"] for item in fallback_targets),
        )
        return fallback_targets

    return []


def _build_chapter_index(outline_chapters: Iterable[dict[str, Any]]) -> list[dict[str, Any]]:
    chapter_index: list[dict[str, Any]] = []
    order_counter = 0

    def _walk(chapters: Iterable[dict[str, Any]], parent_titles: list[str], parent_indices: tuple[int, ...]) -> None:
        nonlocal order_counter
        for local_idx, chapter in enumerate(chapters or []):
            if not isinstance(chapter, dict):
                continue
            chapter_id_raw = chapter.get("id") or chapter.get("title") or f"chapter_{order_counter + 1}"
            chapter_id = str(chapter_id_raw)
            title = str(chapter.get("title") or f"æœªå‘½åç« èŠ‚-{chapter_id[:8]}")
            path_titles = parent_titles + [title]
            entry = {
                "id": chapter_id,
                "id_lower": chapter_id.lower(),
                "title": title,
                "title_lower": title.lower(),
                "path": " > ".join(path_titles),
                "path_lower": " > ".join(part.lower() for part in path_titles),
                "order": order_counter,
                "index": parent_indices + (local_idx,),
            }
            chapter_index.append(entry)
            order_counter += 1
            children = chapter.get("sections") or []
            if children:
                _walk(children, path_titles, entry["index"])

    _walk(outline_chapters or [], [], tuple())
    return chapter_index


def _best_fuzzy_match(text_lower: str, chapters: list[dict[str, Any]]) -> dict[str, Any] | None:
    if not text_lower or not chapters:
        return None
    best_score = 0.0
    best_entry: dict[str, Any] | None = None
    for entry in chapters:
        title_lower = entry["title_lower"]
        if not title_lower:
            continue
        score = SequenceMatcher(None, text_lower, title_lower).ratio()
        if score > best_score:
            best_score = score
            best_entry = entry
    if best_entry and best_score >= 0.6:
        return best_entry
    return None


def _similarity_ratio(left: str, right: str) -> float:
    if not left or not right:
        return 0.0
    return SequenceMatcher(None, left, right).ratio()


def _chapter_body_snippet(draft_content: str, chapter_id: str, max_chars: int = 1600) -> str:
    if not draft_content or not chapter_id:
        return ""
    escaped_id = re.escape(chapter_id)
    pattern = re.compile(
        rf"(^#+.*?<!--\s*section_id:\s*{escaped_id}\s*-->.*?)(?=^#+ |\Z)",
        re.MULTILINE | re.DOTALL,
    )
    match = pattern.search(draft_content)
    if not match:
        return ""
    block = match.group(1).strip()
    lines = block.splitlines()
    if len(lines) > 1:
        body = "\n".join(lines[1:]).strip()
    else:
        body = block
    if not body:
        return ""
    if len(body) <= max_chars:
        return body
    trimmed = body[:max_chars].rstrip()
    return f"{trimmed}\n...[å†…å®¹å·²æˆªæ–­]"


__all__ = ["refine_node"]
